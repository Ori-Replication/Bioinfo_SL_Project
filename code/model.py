from torch_geometric.data import HeteroData
from torch_geometric.nn import HGTConv, Linear,GINConv
import torch_geometric.transforms as T
import torch
import pandas as pd
import numpy as np
from  tqdm import tqdm
import torch.nn.functional as F




class HGT(torch.nn.Module):
    """The Heterogeneous Graph Sampler from the `"Heterogeneous Graph
    Transformer" <https://arxiv.org/abs/2003.01332>`_ paper.
    This loader allows for mini-batch training of GNNs on large-scale graphs
    where full-batch training is not feasible.

    Args:
        data (Any): A :class:`~torch_geometric.data.Data`,
            :class:`~torch_geometric.data.HeteroData`, or
            (:class:`~torch_geometric.data.FeatureStore`,
            :class:`~torch_geometric.data.GraphStore`) data object.
        num_samples (List[int] or Dict[str, List[int]]): The number of nodes to
            sample in each iteration and for each node type.
            If given as a list, will sample the same amount of nodes for each
            node type.
        input_nodes (str or Tuple[str, torch.Tensor]): The indices of nodes for
            which neighbors are sampled to create mini-batches.
            Needs to be passed as a tuple that holds the node type and
            corresponding node indices.
            Node indices need to be either given as a :obj:`torch.LongTensor`
            or :obj:`torch.BoolTensor`.
            If node indices are set to :obj:`None`, all nodes of this specific
            type will be considered.

    """
    def __init__(self, data,hidden_channels, out_channels, num_heads, num_layers):
        super().__init__()

        self.lin_dict = torch.nn.ModuleDict()
        for node_type in data.node_types:
            self.lin_dict[node_type] = Linear(-1, hidden_channels)

        self.convs = torch.nn.ModuleList()
        for _ in range(num_layers):
            conv = HGTConv(hidden_channels, hidden_channels, data.metadata(),
                           num_heads, group='sum')
            self.convs.append(conv)

        self.lin = Linear(hidden_channels, out_channels)

    def forward(self, x_dict, edge_index_dict):
        for node_type, x in x_dict.items():
            x_dict[node_type] = self.lin_dict[node_type](x.float()).relu_()

        for conv in self.convs:
            x_dict = conv(x_dict, edge_index_dict)
        # output node representation 
        for node_type in x_dict.keys():
            x_dict[node_type]=self.lin(x_dict[node_type])
        return x_dict




# class GIN(torch.nn.Module):

    def __init__(self, num_layer, emb_dim, JK = "last", drop_ratio = 0.2, gnn_type='gin'):
        super(GIN, self).__init__()
        self.num_layer = num_layer
        self.drop_ratio = drop_ratio
        self.JK = JK

        if self.num_layer < 2:
            raise ValueError("Number of GNN layers must be greater than 1.")

        ###List of message-passing GNN convs
        self.gnns = torch.nn.ModuleList()

        for layer in range(num_layer):
            if layer == 0:
                input_layer = True
            else:
                input_layer = False


            self.gnns.append(GINConv(emb_dim, aggr = "add", input_layer = input_layer))
            # elif gnn_type == "gin_no_e":
            #     self.gnns.append(GINConv_no_e(emb_dim, aggr = "add", input_layer = input_layer))
            # elif gnn_type == "gin_stand":
            #     self.gnns.append(GINConv_stand(emb_dim, aggr = "add", input_layer = input_layer))
            # elif gnn_type == "gcn":
            #     self.gnns.append(GCNConv(emb_dim,input_layer=input_layer))

    #def forward(self, x, edge_index, edge_attr):
    def forward(self, x, edge_index, edge_attr):
        
        h_list = [x]
        # e_list=[]
        # print(h_list.shape)
        for layer in range(self.num_layer):
            # print(edge_index.dtype)
            # print(edge_index)
            # print(layer)
            h= self.gnns[layer](h_list[layer], edge_index, edge_attr)
            if layer == self.num_layer - 1:
                #remove relu from the last layer
                h = F.dropout(h, self.drop_ratio, training = self.training)
            else:
                h = F.dropout(F.relu(h), self.drop_ratio, training = self.training)
            h_list.append(h)
            # e_list.append(e)
            
        if self.JK == "last":
            node_representation = h_list[-1]
            # edge_representation=e_list[-1]
           
        elif self.JK == "sum":
            h_list = [h.unsqueeze_(0) for h in h_list]
            node_representation = torch.sum(torch.cat(h_list[1:], dim = 0), dim = 0)[0]

        return node_representation