{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m运行具有“bioinfo_3 (Python 3.8.19)”的单元格需要ipykernel包。\n",
      "\u001b[1;31m运行以下命令，将 \"ipykernel\" 安装到 Python 环境中。\n",
      "\u001b[1;31m命令: \"conda install -n bioinfo_3 ipykernel --update-deps --force-reinstall\""
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "from time import time\n",
    "import os\n",
    "import torch_geometric.transforms as T\n",
    "from MyLoader import HeteroDataset\n",
    "from torch_geometric.loader import HGTLoader, NeighborLoader\n",
    "# from dataloader import DataLoaderMasking \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from model import HGT\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "from torch_geometric.datasets import OGB_MAG\n",
    "import torch.nn.init as init\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score,auc,balanced_accuracy_score,cohen_kappa_score,precision_recall_curve\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, EsmModel\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args :\n",
    "    def __init__(self):\n",
    "        self.Full_data_path=r'../data/download_data/kgdata.pkl'\n",
    "        self.node_type='gene/protein'\n",
    "        self.Task_data_path = '../data/original_data/Cell_line_specific'\n",
    "        self.Save_model_path = '../logs_models/train_logs_models/'\n",
    "        self.processed_data_path = '../data/processed_data/'\n",
    "        self.cv = 'CV3'\n",
    "        self.n_fold = 5\n",
    "        self.device = 'cuda:1'\n",
    "        self.do_low_data = False\n",
    "        self.sample_nodes = 1024\n",
    "        self.sample_layers = 4\n",
    "        self.num_workers = 8\n",
    "        self.specific = True # Cell line specific\n",
    "        self.adapted = True # Cell line adapted\n",
    "        self.cell_line_list = ['A375','Jurkat','A549']\n",
    "        self.test_cell_line = 'A549'\n",
    "        self.freeze_graph_encoder = True\n",
    "        self.freeze_esm_encoder = True\n",
    "        self.folds = 5\n",
    "        self.do_train = True\n",
    "        self.train_batch_size = 2\n",
    "        self.test_batch_size = 2\n",
    "        self.hgt_emb_dim = 128\n",
    "        self.hgt_num_heads = 4\n",
    "        self.hgt_dropout_ratio = 0.2\n",
    "        self.hgt_num_layer = 3\n",
    "        self.mlp_hidden_dim = 128\n",
    "        self.lr = 1e-5\n",
    "        self.device_0 = 'cuda:0'\n",
    "        self.device_1 = 'cuda:1'\n",
    "        self.device_2 = 'cuda:2'\n",
    "        self.device_3 = 'cuda:3'\n",
    "        self.esm_sequence_max_length = 512\n",
    "        self.epoch = 100\n",
    "        \n",
    "args=args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_logger(args):\n",
    "    '''\n",
    "    Write logs to checkpoint and console \n",
    "    '''\n",
    "\n",
    "    if args.do_train:\n",
    "        # train_log=str(linear_layer_count)+'_'+args.lr+'_'+'train.log'\n",
    "        log_file = os.path.join(args.Save_model_path or args.init_checkpoint, 'train.log') \n",
    "    else:\n",
    "        log_file = os.path.join(args.Save_model_path or args.init_checkpoint, 'test.log') \n",
    "    \n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s %(levelname)-8s %(message)s', \n",
    "        level=logging.INFO,  # \n",
    "        datefmt='%Y-%m-%d %H:%M:%S', \n",
    "        filename=log_file, \n",
    "        filemode='w'  \n",
    "    )\n",
    "    console = logging.StreamHandler() # \n",
    "    console.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s %(levelname)-8s %(message)s') \n",
    "    console.setFormatter(formatter) \n",
    "    logging.getLogger('').addHandler(console) \n",
    "    \n",
    "def compute_accuracy(target, pred, pred_edge):\n",
    "    target = np.array(target)\n",
    "    pred = np.array(pred)\n",
    "    pred_edge = np.array(pred_edge)\n",
    "    \n",
    "    # 转换为 PyTorch 张量\n",
    "    pred_edge_tensor = torch.tensor(pred_edge, dtype=torch.float32)\n",
    "    scores = torch.softmax(pred_edge_tensor, dim=1).numpy()\n",
    "\n",
    "    \n",
    "    target = target.astype(int)\n",
    "    \n",
    "\n",
    "    \n",
    "    # 计算各项指标\n",
    "    aucu = roc_auc_score(target, scores[:, 1])\n",
    "    precision_tmp, recall_tmp, _thresholds = precision_recall_curve(target, scores[:, 1])\n",
    "    aupr = auc(recall_tmp, precision_tmp)\n",
    "    f1 = f1_score(target, pred)\n",
    "    kappa = cohen_kappa_score(target, pred)\n",
    "    bacc = balanced_accuracy_score(target, pred)\n",
    "    \n",
    "    return aucu, aupr, f1, kappa, bacc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_cell_line_gene_data(args, cell_line):\n",
    "    \"\"\"\n",
    "    load cell line specific gene data\n",
    "    \"\"\"\n",
    "    cell_line_gene_data = pd.read_csv(f\"{args.processed_data_path}/{cell_line}_all_data_gene.csv\")\n",
    "    return cell_line_gene_data\n",
    "\n",
    "def Downstream_data_preprocess(args,n_fold,node_type_dict,cell_line): #FIXME\n",
    "    \"\"\"\n",
    "    load SL data and preprocess before training \n",
    "    \"\"\"\n",
    "    task_data_path=args.Task_data_path\n",
    "    train_data=pd.read_csv(f\"{task_data_path}/{cell_line}/sl_train_{n_fold}.csv\")\n",
    "    test_data=pd.read_csv(f\"{task_data_path}/{cell_line}/sl_test_{n_fold}.csv\",)\n",
    "    train_data.columns=[0,1,2,3]\n",
    "    test_data.columns=[0,1,2,3]\n",
    "    train_data[0]=train_data[0].astype(str).map(node_type_dict)\n",
    "    train_data[1]=train_data[1].astype(str).map(node_type_dict)\n",
    "    test_data[0]=test_data[0].astype(str).map(node_type_dict)\n",
    "    test_data[1]=test_data[1].astype(str).map(node_type_dict)\n",
    "    train_data=train_data.dropna()\n",
    "    test_data=test_data.dropna()\n",
    "    train_data[0]=train_data[0].astype(int)\n",
    "    train_data[1]=train_data[1].astype(int)\n",
    "    test_data[0]=test_data[0].astype(int)\n",
    "    test_data[1]=test_data[1].astype(int)\n",
    "    # low data scenario settings\n",
    "    if args.do_low_data:\n",
    "        num_sample=int(train_data.shape[0]*args.train_data_ratio)\n",
    "        print(num_sample)\n",
    "        train_data=train_data.sample(num_sample,replace=False,random_state=0)\n",
    "        train_data.reset_index(inplace=True)\n",
    "        print(f'train_data.size:{train_data.shape[0]}')\n",
    "\n",
    "    train_node=list(set(train_data[0])|set(train_data[1]))\n",
    "    print(f'train_node.size:{len(train_node)}')\n",
    "    train_mask=torch.zeros((27671))\n",
    "    test_mask=torch.zeros((27671))\n",
    "    test_node=list(set(test_data[0])|set(test_data[1]))\n",
    "    train_mask[train_node]=1\n",
    "    test_mask[test_node]=1\n",
    "    train_mask=train_mask.bool()\n",
    "    test_mask=test_mask.bool()\n",
    "    num_train_node=len(train_node)\n",
    "    num_test_node=len(test_node)\n",
    "    return train_data,test_data,train_mask,test_mask,num_train_node,num_test_node\n",
    "\n",
    "class GenePairDataset(Dataset):\n",
    "    def __init__(self, gene_pairs: pd.DataFrame):\n",
    "        # drop column 2\n",
    "        self.gene_pairs = gene_pairs.drop(columns=2).values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.gene_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.gene_pairs[idx]\n",
    "\n",
    "\n",
    "class sequence_dataset(Dataset):\n",
    "    def __init__(self,sequence_data):\n",
    "        self.sequence_data=sequence_data\n",
    "    def __len__(self):\n",
    "        return len(self.sequence_data)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.sequence_data[idx]\n",
    "\n",
    "def Construct_loader(args,kgdata,cell_line_gene_data,sequence_data,train_mask,test_mask,node_type,train_batch_size,test_batch_size):\n",
    "    \"\"\"\n",
    "    construct loader for train/test data\n",
    "    \"\"\"\n",
    "    \n",
    "    train_loader = HGTLoader(kgdata,\n",
    "    num_samples={key: [args.sample_nodes] * args.sample_layers for key in kgdata.node_types},shuffle=False,\n",
    "    batch_size=train_batch_size,\n",
    "    input_nodes=(node_type,train_mask),num_workers=args.num_workers)\n",
    "\n",
    "    return train_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-05-30 06:56:29,304 INFO     Cell line specific\n",
      "2024-05-30 06:56:29,304 INFO     Cell line specific\n",
      "2024-05-30 06:56:29,304 INFO     Cell line specific\n",
      "2024-05-30 06:56:29,304 INFO     Cell line specific\n",
      "2024-05-30 06:56:29,304 INFO     Cell line specific\n",
      "2024-05-30 06:56:29,304 INFO     Cell line specific\n",
      "2024-05-30 06:56:29,304 INFO     Cell line specific\n",
      "2024-05-30 06:56:29,304 INFO     Cell line specific\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_node.size:319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/54 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "  0%|          | 0/54 [00:47<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 13.94 GiB. GPU 0 has a total capacity of 6.00 GiB of which 3.69 GiB is free. Of the allocated memory 680.18 MiB is allocated by PyTorch, and 639.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 117\u001b[0m\n\u001b[0;32m    115\u001b[0m esm_input \u001b[38;5;241m=\u001b[39m ESM_tokenizer(sequence_batch,padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    116\u001b[0m esm_input\u001b[38;5;241m.\u001b[39mto(args\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 117\u001b[0m sequence_batch_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mESM_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mesm_input\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpooler_output\n\u001b[0;32m    118\u001b[0m ESM_nodea_emb \u001b[38;5;241m=\u001b[39m sequence_batch_embedding[:\u001b[38;5;28mlen\u001b[39m(node_a)]\n\u001b[0;32m    119\u001b[0m ESM_nodeb_emb \u001b[38;5;241m=\u001b[39m sequence_batch_embedding[\u001b[38;5;28mlen\u001b[39m(node_a):]\n",
      "File \u001b[1;32md:\\Applications\\Miniconda3\\envs\\metagpt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Applications\\Miniconda3\\envs\\metagpt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Applications\\Miniconda3\\envs\\metagpt\\Lib\\site-packages\\transformers\\models\\esm\\modeling_esm.py:910\u001b[0m, in \u001b[0;36mEsmModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    901\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    903\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    904\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    905\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    909\u001b[0m )\n\u001b[1;32m--> 910\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    911\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    922\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    923\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Applications\\Miniconda3\\envs\\metagpt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Applications\\Miniconda3\\envs\\metagpt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Applications\\Miniconda3\\envs\\metagpt\\Lib\\site-packages\\transformers\\models\\esm\\modeling_esm.py:615\u001b[0m, in \u001b[0;36mEsmEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    604\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    605\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    606\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    612\u001b[0m         output_attentions,\n\u001b[0;32m    613\u001b[0m     )\n\u001b[0;32m    614\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 615\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    625\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32md:\\Applications\\Miniconda3\\envs\\metagpt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Applications\\Miniconda3\\envs\\metagpt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Applications\\Miniconda3\\envs\\metagpt\\Lib\\site-packages\\transformers\\models\\esm\\modeling_esm.py:505\u001b[0m, in \u001b[0;36mEsmLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    494\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    495\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    502\u001b[0m ):\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    504\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 505\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    512\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32md:\\Applications\\Miniconda3\\envs\\metagpt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Applications\\Miniconda3\\envs\\metagpt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Applications\\Miniconda3\\envs\\metagpt\\Lib\\site-packages\\transformers\\models\\esm\\modeling_esm.py:439\u001b[0m, in \u001b[0;36mEsmAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    430\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    436\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    437\u001b[0m ):\n\u001b[0;32m    438\u001b[0m     hidden_states_ln \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states)\n\u001b[1;32m--> 439\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states_ln\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    448\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    449\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32md:\\Applications\\Miniconda3\\envs\\metagpt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Applications\\Miniconda3\\envs\\metagpt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Applications\\Miniconda3\\envs\\metagpt\\Lib\\site-packages\\transformers\\models\\esm\\modeling_esm.py:343\u001b[0m, in \u001b[0;36mEsmSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    340\u001b[0m     query_layer, key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_embeddings(query_layer, key_layer)\n\u001b[0;32m    342\u001b[0m \u001b[38;5;66;03m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[1;32m--> 343\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelative_key\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelative_key_query\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    346\u001b[0m     seq_length \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 13.94 GiB. GPU 0 has a total capacity of 6.00 GiB of which 3.69 GiB is free. Of the allocated memory 680.18 MiB is allocated by PyTorch, and 639.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# main\n",
    "set_logger(args)\n",
    "with open (args.Full_data_path,'rb') as f:\n",
    "    kgdata=pickle.load(f)\n",
    "with open(\"../data/processed_data/gene_protein_2_id.json\",'rb') as f:\n",
    "    node_index=json.load(f)\n",
    "sequence_data = pd.read_csv('../data/train_data/uniprot_results_filtered.csv')\n",
    "sequence_data['Gene_id'] = sequence_data['Gene Name'].map(node_index['gene/protein'])\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "device = torch.device(args.device_0 )\n",
    "device_0 = torch.device(args.device_0)\n",
    "device_1 = torch.device(args.device_1)\n",
    "device_2 = torch.device(args.device_2)\n",
    "device_3 = torch.device(args.device_3)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "\n",
    "gene_protein=node_index[args.node_type] \n",
    "eval_metric_folds={'fold':[],'auc':[],'aupr':[],'f1':[],'bacc':[],'kappa':[]}\n",
    "node_type=args.node_type\n",
    "num_nodes_type=len(kgdata.node_types)\n",
    "num_edge_type=len(kgdata.edge_types)\n",
    "num_nodes=kgdata.num_nodes\n",
    "input_node_embeddings = torch.nn.Embedding(num_nodes_type, 16)\n",
    "torch.nn.init.xavier_uniform_(input_node_embeddings.weight.data)\n",
    "for i in range(len(kgdata.node_types)):\n",
    "    num_repeat=kgdata[kgdata.node_types[i]].x.shape[0]\n",
    "    kgdata[kgdata.node_types[i]].x =input_node_embeddings(torch.tensor(i)).repeat([num_repeat,1]).detach()\n",
    "\n",
    "HGT_model = HGT(kgdata,2*args.hgt_emb_dim,args.hgt_emb_dim,args.hgt_num_heads,args.hgt_num_layer).to(args.device_0)\n",
    "# freeze\n",
    "if args.freeze_graph_encoder:\n",
    "    for param in HGT_model.parameters():\n",
    "        param.requires_grad = False\n",
    "ESM_tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "ESM_model = EsmModel.from_pretrained(\"facebook/esm2_t6_8M_UR50D\").to(args.device_3)\n",
    "# freeze\n",
    "if args.freeze_esm_encoder:\n",
    "    for param in ESM_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "mlp = nn.Sequential(\n",
    "                nn.Linear(2*args.hgt_emb_dim+2*320+2*3, args.mlp_hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(args.mlp_hidden_dim, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32, 2),\n",
    "                ).to(args.device_2)\n",
    "\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=args.lr)\n",
    "\n",
    "if args.specific:# Do cv within cell line\n",
    "    logging.info(f\"Cell line specific\")\n",
    "    for cell_line in args.cell_line_list:\n",
    "        cell_line_gene_data = load_cell_line_gene_data(args, cell_line) \n",
    "        cell_line_gene_data['Gene_id'] = cell_line_gene_data['Gene Name'].map(node_index['gene/protein'])   \n",
    "        for fold in range(args.folds):\n",
    "            n_fold = fold\n",
    "            train_data,test_data,train_mask,test_mask,num_train_node,num_test_node=Downstream_data_preprocess(args,n_fold,gene_protein,cell_line)\n",
    "            loss_sum = 0\n",
    "            aucu_sum=0\n",
    "            f1_sum=0\n",
    "            bacc_sum=0\n",
    "            kappa_sum=0\n",
    "            aupr_sum=0\n",
    "            edge_used=[]\n",
    "             # map gene name(column name) to gene id\n",
    "            training_logs = []\n",
    "            testing_logs=[]\n",
    "            prediction_result_log_fold=[]\n",
    "            label_log_fold = []\n",
    "        \n",
    "            auc_sum_fold=[]\n",
    "            aupr_sum_fold=[]\n",
    "            f1_sum_fold=[]\n",
    "            bacc_sum_fold=[]\n",
    "            kappa_sum_fold=[]\n",
    "            for s in tqdm(range(args.epoch)):\n",
    "                gene_pair_loader = DataLoader(GenePairDataset(train_data), batch_size=args.train_batch_size, shuffle=True)\n",
    "                # Train\n",
    "                prediction_result_log_epoch=[]\n",
    "                label_log_epoch = []\n",
    "                for step,batch in enumerate(tqdm(gene_pair_loader)):\n",
    "                    node_a = batch[:, 0]\n",
    "                    node_b = batch[:, 1]\n",
    "                    node = torch.cat([node_a, node_b], dim=0)\n",
    "                    label = batch[:, 2].to(args.device_2)\n",
    "                    node_set = set(node_a.numpy()) | set(node_b.numpy())\n",
    "                    unique_node = list(node_set)\n",
    "                    len_unique_node = len(unique_node)\n",
    "                    node_mask = torch.zeros((27671)) # The number of gene/protein nodes in kg\n",
    "                    node_mask[unique_node] = 1\n",
    "                    node_mask = node_mask.bool()\n",
    "                    # find corresponding sequence data of node\n",
    "                    sequence_batch = []\n",
    "                    for one_node in node:\n",
    "                        sequence = sequence_data[sequence_data['Gene_id'] == one_node.item()]['Sequence'].values[0]\n",
    "                        # cut\n",
    "                        sequence = sequence[:args.esm_sequence_max_length]\n",
    "                        sequence_batch.append(sequence_data[sequence_data['Gene_id'] == one_node.item()]['Sequence'].values[0])\n",
    "                    \n",
    "                    kg_loader = HGTLoader(kgdata,\n",
    "                        num_samples={key: [args.sample_nodes] * args.sample_layers for key in kgdata.node_types},\n",
    "                        shuffle=False,\n",
    "                        batch_size=len_unique_node,\n",
    "                        input_nodes=(node_type,node_mask),\n",
    "                        num_workers=args.num_workers) # FIXME 这里效率瓶颈，每次都要重新构建loader，可以考虑提前构建好...吗？\n",
    "                    # get the whole batch of kg_loader\n",
    "                    \n",
    "                    \n",
    "                    # HGT forward\n",
    "                    for kg_batch in kg_loader:\n",
    "                        break\n",
    "                    kg_batch.to(args.device_0)\n",
    "                    node_rep= HGT_model(kg_batch.x_dict, kg_batch.edge_index_dict)\n",
    "                    node_rep=node_rep[node_type]\n",
    "                    node_set=pd.DataFrame(list(kg_batch[node_type].n_id[:len_unique_node].squeeze().detach().cpu().numpy()))\n",
    "                    node_set.drop_duplicates(inplace=True,keep='first')\n",
    "                    node_set[1]=range(node_set.shape[0])\n",
    "                    node_map=dict(zip(node_set[0],node_set[1]))\n",
    "                    # batch to pandas\n",
    "                    batch=pd.DataFrame(batch.numpy())\n",
    "                    # column name 0,1,2\n",
    "                    prediction_edge=batch[[0,1]]\n",
    "                    prediction_label=batch[2]\n",
    "                    edge_used.append(prediction_edge.shape[0])\n",
    "                    edge_a,edge_b=prediction_edge[0],prediction_edge[1]\n",
    "                    edge_a=edge_a.map(node_map)\n",
    "                    edge_b=edge_b.map(node_map)\n",
    "                    HGT_nodea_emb=node_rep[edge_a.values]\n",
    "                    HGT_nodeb_emb=node_rep[edge_b.values]\n",
    "                    \n",
    "                    # ESM forward\n",
    "                    # tokenize sequence\n",
    "                    esm_input = ESM_tokenizer(sequence_batch,padding = True,truncation=True,return_tensors='pt')\n",
    "                    esm_input.to(args.device_3)\n",
    "                    sequence_batch_embedding = ESM_model(**esm_input).pooler_output\n",
    "                    ESM_nodea_emb = sequence_batch_embedding[:len(node_a)]\n",
    "                    ESM_nodeb_emb = sequence_batch_embedding[len(node_a):]\n",
    "                    \n",
    "                    cell_line_gene_data_nodea = []\n",
    "                    for one_node in node_a:\n",
    "                        # 使用loc来选择特定列并进行条件筛选\n",
    "                        selected_data = cell_line_gene_data.loc[cell_line_gene_data['Gene_id'] == one_node.item(), ['CN', 'Expression', 'HotspotMutation']]\n",
    "                        \n",
    "                        # 将选中的数据转换为列表并添加到结果列表中\n",
    "                        cell_line_gene_data_nodea.append(selected_data.values.tolist())\n",
    "                    cell_line_gene_data_nodea_embedding = torch.tensor(np.array(cell_line_gene_data_nodea).squeeze())\n",
    "                    cell_line_gene_data_nodeb = []\n",
    "                    for one_node in node_b:\n",
    "                        selected_data = cell_line_gene_data.loc[cell_line_gene_data['Gene_id'] == one_node.item(), ['CN', 'Expression', 'HotspotMutation']]\n",
    "                        cell_line_gene_data_nodeb.append(selected_data.values.tolist())\n",
    "                    cell_line_gene_data_nodeb_embedding = torch.tensor(np.array(cell_line_gene_data_nodeb).squeeze())\n",
    "                    HGT_nodea_emb = HGT_nodea_emb.to(args.device_2)\n",
    "                    HGT_nodeb_emb = HGT_nodeb_emb.to(args.device_2)                \n",
    "                    ESM_nodea_emb = ESM_nodea_emb.to(args.device_2)\n",
    "                    ESM_nodeb_emb = ESM_nodeb_emb.to(args.device_2)\n",
    "                    cell_line_gene_data_nodea_embedding = cell_line_gene_data_nodea_embedding.to(args.device_2)\n",
    "                    cell_line_gene_data_nodeb_embedding = cell_line_gene_data_nodeb_embedding.to(args.device_2)\n",
    "\n",
    "                    nodea_embedding = torch.cat([HGT_nodea_emb, ESM_nodea_emb, cell_line_gene_data_nodea_embedding], dim=1)\n",
    "                    nodeb_embedding = torch.cat([HGT_nodeb_emb, ESM_nodeb_emb, cell_line_gene_data_nodeb_embedding], dim=1)\n",
    "                    # embedding to float\n",
    "                    edge_embedding = torch.cat([nodea_embedding, nodeb_embedding], dim=1).float()\n",
    "\n",
    "                    embedding_dim = edge_embedding.shape[1]\n",
    "                    prediction_result = mlp(edge_embedding)\n",
    "                    prediction_result_log_epoch.append(prediction_result.detach().cpu().numpy())\n",
    "                    label_log.append(label.tolist())\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    criterion = nn.CrossEntropyLoss()\n",
    "                    loss = criterion(prediction_result, label)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    print(loss)\n",
    "                    \n",
    "                    if step == 4:\n",
    "                        break\n",
    "                # prediction_result = prediction_result.flatten() \n",
    "                # prediction_result_log: (Step, Batch, 2) -> (Step * Batch, 2)\n",
    "                prediction_result_log_epoch = np.concatenate(prediction_result_log_epoch)\n",
    "                label_log = np.array(label_log).flatten()\n",
    "                aucu, aupr, f1, kappa, bacc = compute_accuracy(label_log, np.array(prediction_result_log_epoch).argmax(axis=1), prediction_result_log_epoch)\n",
    "                auc_sum_fold.append(aucu)\n",
    "                aupr_sum_fold.append(aupr)\n",
    "                f1_sum_fold.append(f1)\n",
    "                bacc_sum_fold.append(bacc)\n",
    "                kappa_sum_fold.append(kappa)\n",
    "                logging.info(f\"Step {step}, Loss: {loss.item()}, AUC: {aucu}, AUPR: {aupr}, F1: {f1}, Kappa: {kappa}, BAcc: {bacc}\")\n",
    "                print(f\"Step {step}, Loss: {loss.item()}, AUC: {aucu}, AUPR: {aupr}, F1: {f1}, Kappa: {kappa}, BAcc: {bacc}\")\n",
    "            prediction_result_log_fold.append(prediction_result_log_epoch)\n",
    "            label_log_fold.append(label_log)\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_line_gene_data_nodea = []\n",
    "for one_node in node_a:\n",
    "    # 使用loc来选择特定列并进行条件筛选\n",
    "    selected_data = cell_line_gene_data.loc[cell_line_gene_data['Gene_id'] == one_node.item(), ['CN', 'Expression', 'HotspotMutation']]\n",
    "    \n",
    "    # 将选中的数据转换为列表并添加到结果列表中\n",
    "    cell_line_gene_data_nodea.append(selected_data.values.tolist())\n",
    "cell_line_gene_data_nodea = torch.tensor(np.array(cell_line_gene_data_nodea).squeeze())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6762, 3.1683, 0.0000],\n",
       "        [0.8269, 7.4974, 0.0000],\n",
       "        [1.0403, 4.9860, 0.0000],\n",
       "        [1.0403, 4.9860, 0.0000],\n",
       "        [1.1114, 8.0649, 0.0000],\n",
       "        [1.0288, 7.0466, 0.0000],\n",
       "        [0.7803, 9.9216, 0.0000],\n",
       "        [1.0288, 0.0000, 0.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_line_gene_data_nodea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0500,  0.1779,  0.3884,  ...,  0.6528, -0.0795, -0.1957],\n",
       "         [ 0.1944,  0.4400, -0.1651,  ...,  0.2236, -0.1313,  0.2660],\n",
       "         [-0.1316,  0.0507, -0.1536,  ...,  0.0297,  0.1301,  0.3650],\n",
       "         ...,\n",
       "         [-0.0488, -0.4784,  0.0021,  ...,  0.3245, -0.1609,  0.0787],\n",
       "         [-0.0733, -0.4250, -0.1079,  ...,  0.2768, -0.1620,  0.0026],\n",
       "         [-0.1057, -0.3860,  0.1714,  ...,  0.2503, -0.2435, -0.0530]],\n",
       "\n",
       "        [[ 0.0984,  0.4506,  0.1361,  ...,  1.0224, -0.0645, -0.4918],\n",
       "         [ 0.1701, -0.1031, -0.0250,  ...,  0.5356, -0.1969, -0.1501],\n",
       "         [-0.2535, -0.6615,  0.0955,  ...,  0.1602,  0.0553,  0.0669],\n",
       "         ...,\n",
       "         [ 0.0504, -0.3176,  0.1794,  ...,  0.0690, -0.1398,  0.0064],\n",
       "         [ 0.0590, -0.3585,  0.1690,  ...,  0.0748, -0.1159, -0.0578],\n",
       "         [ 0.0317, -0.2721,  0.1833,  ...,  0.1854, -0.0728, -0.0046]],\n",
       "\n",
       "        [[ 0.0496,  0.4996,  0.1896,  ...,  0.9381, -0.1036, -0.4078],\n",
       "         [ 0.2482,  0.2525, -0.3222,  ...,  0.7328, -0.2753, -0.2601],\n",
       "         [-0.0992, -0.4754, -0.3919,  ...,  0.4380,  0.1895,  0.0646],\n",
       "         ...,\n",
       "         [ 0.0015, -0.1955,  0.1146,  ...,  0.4282, -0.3125, -0.2090],\n",
       "         [ 0.0110, -0.2265,  0.0497,  ...,  0.3957, -0.2347, -0.2262],\n",
       "         [-0.0192, -0.2945,  0.0285,  ...,  0.3478, -0.2110, -0.1954]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0157,  0.3994, -0.0386,  ...,  1.1074, -0.1355, -0.5472],\n",
       "         [ 0.2573,  0.0129, -0.1252,  ...,  0.7690, -0.0553, -0.3596],\n",
       "         [-0.2849, -0.5293,  0.0050,  ...,  0.3445,  0.0604, -0.0943],\n",
       "         ...,\n",
       "         [-0.0032, -0.1823,  0.0625,  ...,  0.6247, -0.1887, -0.1860],\n",
       "         [-0.0909, -0.2490,  0.1957,  ...,  0.4916, -0.1548, -0.1235],\n",
       "         [-0.0589, -0.2542,  0.1818,  ...,  0.5324, -0.0595, -0.2024]],\n",
       "\n",
       "        [[-0.1752,  0.5944,  0.2774,  ...,  1.1528, -0.1415, -0.2497],\n",
       "         [-0.1252,  0.2487, -0.1998,  ...,  0.8297, -0.1379, -0.0238],\n",
       "         [-0.0614,  0.0353, -0.1087,  ...,  0.2465, -0.0849,  0.0917],\n",
       "         ...,\n",
       "         [ 0.0026, -0.2268,  0.1519,  ...,  0.6016, -0.1084, -0.0460],\n",
       "         [ 0.0605,  0.1093,  0.2385,  ...,  0.4190, -0.0120, -0.1442],\n",
       "         [-0.0303,  0.0859,  0.2588,  ...,  0.2123, -0.0400, -0.0986]],\n",
       "\n",
       "        [[ 0.0501,  0.7459,  0.3510,  ...,  0.9041, -0.1591, -0.4368],\n",
       "         [ 0.2872,  0.0479, -0.2996,  ...,  0.6525, -0.2783, -0.0912],\n",
       "         [-0.0538, -0.4619, -0.0241,  ..., -0.2935, -0.2214,  0.2263],\n",
       "         ...,\n",
       "         [ 0.0199, -0.0298,  0.4013,  ...,  0.2814, -0.0987,  0.0090],\n",
       "         [ 0.0810, -0.1023,  0.2336,  ...,  0.5157, -0.2579, -0.1199],\n",
       "         [-0.0201, -0.1528,  0.1832,  ...,  0.5161, -0.1810, -0.2147]]],\n",
       "       device='cuda:0'), pooler_output=tensor([[ 0.0334,  0.1576, -0.2025,  ...,  0.0637, -0.0576,  0.1701],\n",
       "        [ 0.2020,  0.2187, -0.1212,  ...,  0.0675, -0.0981,  0.0873],\n",
       "        [ 0.1707,  0.1756, -0.1861,  ...,  0.0064, -0.0272,  0.0058],\n",
       "        ...,\n",
       "        [ 0.1821,  0.2450, -0.2048,  ..., -0.0233, -0.0464,  0.0629],\n",
       "        [ 0.1072,  0.2134, -0.1191,  ...,  0.0675, -0.1610,  0.1101],\n",
       "        [ 0.1201,  0.1530, -0.1544,  ...,  0.0414, -0.1354,  0.1123]],\n",
       "       device='cuda:0'), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ESM_model(**esm_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metagpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
