{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "from time import time\n",
    "import os\n",
    "import torch_geometric.transforms as T\n",
    "from MyLoader import HeteroDataset\n",
    "from torch_geometric.loader import HGTLoader, NeighborLoader\n",
    "# from dataloader import DataLoaderMasking \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from model import HGT\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "from torch_geometric.datasets import OGB_MAG\n",
    "import torch.nn.init as init\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score,auc,balanced_accuracy_score,cohen_kappa_score,precision_recall_curve\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, EsmModel\n",
    "import joblib\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args :\n",
    "    def __init__(self):\n",
    "        self.Full_data_path=r'../data/download_data/kgdata.pkl'\n",
    "        self.node_type='gene/protein'\n",
    "        self.Task_data_path = '../data/original_data/Cell_line_specific'\n",
    "        self.Save_model_path = '../logs_models/train_logs_models/'\n",
    "        self.processed_data_path = '../data/processed_data/'\n",
    "        self.cv = 'CV3'\n",
    "        self.n_fold = 5\n",
    "        self.device = 'cuda:1'\n",
    "        self.do_low_data = False\n",
    "        self.sample_nodes = 1024\n",
    "        self.sample_layers = 4\n",
    "        self.num_workers = 8\n",
    "        self.specific = True # Cell line specific\n",
    "        self.adapted = True # Cell line adapted\n",
    "        self.cell_line_list = ['A375','Jurkat','A549']\n",
    "        self.test_cell_line = 'A549'\n",
    "        self.freeze_graph_encoder = True\n",
    "        self.freeze_esm_encoder = True\n",
    "        self.folds = 5\n",
    "        self.do_train = True\n",
    "        self.train_batch_size = 128\n",
    "        self.test_batch_size = 128\n",
    "        self.hgt_emb_dim = 128\n",
    "        self.hgt_num_heads = 4\n",
    "        self.hgt_dropout_ratio = 0.2\n",
    "        self.hgt_num_layer = 3\n",
    "        self.mlp_hidden_dim = 128\n",
    "        self.lr = 1e-5\n",
    "        self.device_0 = 'cuda:0'\n",
    "        self.device_1 = 'cuda:1'\n",
    "        self.device_2 = 'cuda:2'\n",
    "        self.device_3 = 'cuda:3'\n",
    "        self.esm_sequence_max_length = 256\n",
    "        self.epoch = 100\n",
    "        self.use_esm_embedding = True\n",
    "        self.esm_embedding_file = '../data/download_data/gene_esm2emb.pkl'\n",
    "        \n",
    "args=args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_logger(args):\n",
    "    '''\n",
    "    Write logs to checkpoint and console \n",
    "    '''\n",
    "\n",
    "    if args.do_train:\n",
    "        # train_log=str(linear_layer_count)+'_'+args.lr+'_'+'train.log'\n",
    "        log_file = os.path.join(args.Save_model_path or args.init_checkpoint, 'train.log') \n",
    "    else:\n",
    "        log_file = os.path.join(args.Save_model_path or args.init_checkpoint, 'test.log') \n",
    "    \n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s %(levelname)-8s %(message)s', \n",
    "        level=logging.INFO,  # \n",
    "        datefmt='%Y-%m-%d %H:%M:%S', \n",
    "        filename=log_file, \n",
    "        filemode='w'  \n",
    "    )\n",
    "    console = logging.StreamHandler() # \n",
    "    console.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s %(levelname)-8s %(message)s') \n",
    "    console.setFormatter(formatter) \n",
    "    logging.getLogger('').addHandler(console) \n",
    "    \n",
    "def compute_accuracy(target, pred, pred_edge):\n",
    "    target = np.array(target)\n",
    "    pred = np.array(pred)\n",
    "    pred_edge = np.array(pred_edge)\n",
    "    \n",
    "    # 转换为 PyTorch 张量\n",
    "    pred_edge_tensor = torch.tensor(pred_edge, dtype=torch.float32)\n",
    "    scores = torch.softmax(pred_edge_tensor, dim=1).numpy()\n",
    "\n",
    "    \n",
    "    target = target.astype(int)\n",
    "    \n",
    "\n",
    "    \n",
    "    # 计算各项指标\n",
    "    aucu = roc_auc_score(target, scores[:, 1])\n",
    "    precision_tmp, recall_tmp, _thresholds = precision_recall_curve(target, scores[:, 1])\n",
    "    aupr = auc(recall_tmp, precision_tmp)\n",
    "    f1 = f1_score(target, pred)\n",
    "    kappa = cohen_kappa_score(target, pred)\n",
    "    bacc = balanced_accuracy_score(target, pred)\n",
    "    \n",
    "    return aucu, aupr, f1, kappa, bacc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_cell_line_gene_data(args, cell_line):\n",
    "    \"\"\"\n",
    "    load cell line specific gene data\n",
    "    \"\"\"\n",
    "    cell_line_gene_data = pd.read_csv(f\"{args.processed_data_path}/{cell_line}_all_data_gene.csv\")\n",
    "    return cell_line_gene_data\n",
    "\n",
    "def load_esm_embedding_data(args, node_index_data):\n",
    "    esm_embedding = joblib.load(args.esm_embedding_file )\n",
    "    esm_embedding_geneid = {}\n",
    "    for key, value in esm_embedding.items():\n",
    "        if key not in node_index_data['gene/protein']:\n",
    "            mapped_key = key  # Use original key or a placeholder if needed\n",
    "            esm_embedding_geneid[mapped_key] = torch.zeros(1280)\n",
    "        else:\n",
    "            mapped_key = node_index_data['gene/protein'][key]\n",
    "            esm_embedding_geneid[mapped_key] = value\n",
    "    return esm_embedding_geneid\n",
    "\n",
    "\n",
    "def Downstream_data_preprocess(args,n_fold,node_type_dict,cell_line): #FIXME\n",
    "    \"\"\"\n",
    "    load SL data and preprocess before training \n",
    "    \"\"\"\n",
    "    task_data_path=args.Task_data_path\n",
    "    train_data=pd.read_csv(f\"{task_data_path}/{cell_line}/sl_train_{n_fold}.csv\")\n",
    "    test_data=pd.read_csv(f\"{task_data_path}/{cell_line}/sl_test_{n_fold}.csv\",)\n",
    "    train_data.columns=[0,1,2,3]\n",
    "    test_data.columns=[0,1,2,3]\n",
    "    train_data[0]=train_data[0].astype(str).map(node_type_dict)\n",
    "    train_data[1]=train_data[1].astype(str).map(node_type_dict)\n",
    "    test_data[0]=test_data[0].astype(str).map(node_type_dict)\n",
    "    test_data[1]=test_data[1].astype(str).map(node_type_dict)\n",
    "    train_data=train_data.dropna()\n",
    "    test_data=test_data.dropna()\n",
    "    train_data[0]=train_data[0].astype(int)\n",
    "    train_data[1]=train_data[1].astype(int)\n",
    "    test_data[0]=test_data[0].astype(int)\n",
    "    test_data[1]=test_data[1].astype(int)\n",
    "    # low data scenario settings\n",
    "    if args.do_low_data:\n",
    "        num_sample=int(train_data.shape[0]*args.train_data_ratio)\n",
    "        print(num_sample)\n",
    "        train_data=train_data.sample(num_sample,replace=False,random_state=0)\n",
    "        train_data.reset_index(inplace=True)\n",
    "        print(f'train_data.size:{train_data.shape[0]}')\n",
    "\n",
    "    train_node=list(set(train_data[0])|set(train_data[1]))\n",
    "    print(f'train_node.size:{len(train_node)}')\n",
    "    train_mask=torch.zeros((27671))\n",
    "    test_mask=torch.zeros((27671))\n",
    "    test_node=list(set(test_data[0])|set(test_data[1]))\n",
    "    train_mask[train_node]=1\n",
    "    test_mask[test_node]=1\n",
    "    train_mask=train_mask.bool()\n",
    "    test_mask=test_mask.bool()\n",
    "    num_train_node=len(train_node)\n",
    "    num_test_node=len(test_node)\n",
    "    return train_data,test_data,train_mask,test_mask,num_train_node,num_test_node\n",
    "\n",
    "\n",
    "\n",
    "class GenePairDataset(Dataset):\n",
    "    def __init__(self, gene_pairs: pd.DataFrame):\n",
    "        # drop column 2\n",
    "        self.gene_pairs = gene_pairs.drop(columns=2).values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.gene_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.gene_pairs[idx]\n",
    "\n",
    "\n",
    "class sequence_dataset(Dataset):\n",
    "    def __init__(self,sequence_data):\n",
    "        self.sequence_data=sequence_data\n",
    "    def __len__(self):\n",
    "        return len(self.sequence_data)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.sequence_data[idx]\n",
    "\n",
    "def Construct_loader(args,kgdata,cell_line_gene_data,sequence_data,train_mask,test_mask,node_type,train_batch_size,test_batch_size):\n",
    "    \"\"\"\n",
    "    construct loader for train/test data\n",
    "    \"\"\"\n",
    "    \n",
    "    train_loader = HGTLoader(kgdata,\n",
    "    num_samples={key: [args.sample_nodes] * args.sample_layers for key in kgdata.node_types},shuffle=False,\n",
    "    batch_size=train_batch_size,\n",
    "    input_nodes=(node_type,train_mask),num_workers=args.num_workers)\n",
    "\n",
    "    return train_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The NVIDIA driver on your system is too old (found version 11000). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m     num_repeat\u001b[38;5;241m=\u001b[39mkgdata[kgdata\u001b[38;5;241m.\u001b[39mnode_types[i]]\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     29\u001b[0m     kgdata[kgdata\u001b[38;5;241m.\u001b[39mnode_types[i]]\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39minput_node_embeddings(torch\u001b[38;5;241m.\u001b[39mtensor(i))\u001b[38;5;241m.\u001b[39mrepeat([num_repeat,\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m---> 31\u001b[0m HGT_model \u001b[38;5;241m=\u001b[39m \u001b[43mHGT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkgdata\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhgt_emb_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhgt_emb_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhgt_num_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhgt_num_layer\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# freeze\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mfreeze_graph_encoder:\n",
      "File \u001b[0;32m~/anaconda3/envs/bioinfo_3/lib/python3.8/site-packages/torch/nn/modules/module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/bioinfo_3/lib/python3.8/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bioinfo_3/lib/python3.8/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bioinfo_3/lib/python3.8/site-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bioinfo_3/lib/python3.8/site-packages/torch/nn/modules/module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1154\u001b[0m             device,\n\u001b[1;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1156\u001b[0m             non_blocking,\n\u001b[1;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1158\u001b[0m         )\n\u001b[0;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/bioinfo_3/lib/python3.8/site-packages/torch/nn/parameter.py:158\u001b[0m, in \u001b[0;36mUninitializedTensorMixin.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempted to use an uninitialized parameter in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis error happens when you are using a `LazyModule` or \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexplicitly manipulating `torch.nn.parameter.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjects. When using LazyModules Call `forward` with a dummy batch \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto initialize the parameters before calling torch functions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/bioinfo_3/lib/python3.8/site-packages/torch/cuda/__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    292\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 293\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    297\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The NVIDIA driver on your system is too old (found version 11000). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver."
     ]
    }
   ],
   "source": [
    "# main\n",
    "set_logger(args)\n",
    "with open (args.Full_data_path,'rb') as f:\n",
    "    kgdata=pickle.load(f)\n",
    "with open(\"../data/processed_data/gene_protein_2_id.json\",'rb') as f:\n",
    "    node_index=json.load(f)\n",
    "sequence_data = pd.read_csv('../data/train_data/uniprot_results_filtered.csv')\n",
    "sequence_data['Gene_id'] = sequence_data['Gene Name'].map(node_index['gene/protein'])\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "device = torch.device(args.device_0 )\n",
    "device_0 = torch.device(args.device_0)\n",
    "device_1 = torch.device(args.device_1)\n",
    "device_2 = torch.device(args.device_2)\n",
    "device_3 = torch.device(args.device_3)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "\n",
    "gene_protein=node_index[args.node_type] \n",
    "eval_metric_folds={'fold':[],'auc':[],'aupr':[],'f1':[],'bacc':[],'kappa':[]}\n",
    "node_type=args.node_type\n",
    "num_nodes_type=len(kgdata.node_types)\n",
    "num_edge_type=len(kgdata.edge_types)\n",
    "num_nodes=kgdata.num_nodes\n",
    "input_node_embeddings = torch.nn.Embedding(num_nodes_type, 16)\n",
    "torch.nn.init.xavier_uniform_(input_node_embeddings.weight.data)\n",
    "for i in range(len(kgdata.node_types)):\n",
    "    num_repeat=kgdata[kgdata.node_types[i]].x.shape[0]\n",
    "    kgdata[kgdata.node_types[i]].x =input_node_embeddings(torch.tensor(i)).repeat([num_repeat,1]).detach()\n",
    "\n",
    "HGT_model = HGT(kgdata,2*args.hgt_emb_dim,args.hgt_emb_dim,args.hgt_num_heads,args.hgt_num_layer).to(args.device_0)\n",
    "# HGT_model = nn.DataParallel(HGT_model, device_ids=[args.device_0, args.device_1, args.device_2, args.device_3]) \n",
    "# freeze\n",
    "# if args.freeze_graph_encoder:\n",
    "#     for param in HGT_model.parameters():\n",
    "#         param.requires_grad = False\n",
    "        \n",
    "\n",
    "if not args.use_esm_embedding:\n",
    "    ESM_tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "    ESM_model = EsmModel.from_pretrained(\"facebook/esm2_t6_8M_UR50D\").to(args.device_3)\n",
    "    ESM_model = nn.DataParallel(ESM_model, device_ids=[args.device_3, args.device_2, args.device_1])  # 使用两个设备\n",
    "    # freeze\n",
    "    if args.freeze_esm_encoder:\n",
    "        for param in ESM_model.parameters():\n",
    "            param.requires_grad = False\n",
    "else:\n",
    "    esm_embedding_geneid = load_esm_embedding_data(args, node_index)\n",
    "\n",
    "mlp = nn.Sequential(\n",
    "                nn.Linear(2*args.hgt_emb_dim+2*320+2*3, args.mlp_hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(args.mlp_hidden_dim, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32, 2),\n",
    "                ).to(args.device_2)\n",
    "\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=args.lr)\n",
    "\n",
    "if args.specific:# Do cv within cell line\n",
    "    logging.info(f\"Cell line specific\")\n",
    "    for cell_line in args.cell_line_list:\n",
    "        cell_line_gene_data = load_cell_line_gene_data(args, cell_line) \n",
    "        cell_line_gene_data['Gene_id'] = cell_line_gene_data['Gene Name'].map(node_index['gene/protein'])   \n",
    "        for fold in range(args.folds):\n",
    "            n_fold = fold\n",
    "            train_data,test_data,train_mask,test_mask,num_train_node,num_test_node=Downstream_data_preprocess(args,n_fold,gene_protein,cell_line)\n",
    "            loss_sum = 0\n",
    "            aucu_sum=0\n",
    "            f1_sum=0\n",
    "            bacc_sum=0\n",
    "            kappa_sum=0\n",
    "            aupr_sum=0\n",
    "            edge_used=[]\n",
    "             # map gene name(column name) to gene id\n",
    "            training_logs = []\n",
    "            testing_logs=[]\n",
    "            prediction_result_log_fold=[]\n",
    "            label_log_fold = []\n",
    "        \n",
    "            auc_sum_fold=[]\n",
    "            aupr_sum_fold=[]\n",
    "            f1_sum_fold=[]\n",
    "            bacc_sum_fold=[]\n",
    "            kappa_sum_fold=[]\n",
    "            for s in tqdm(range(args.epoch)):\n",
    "                gene_pair_loader = DataLoader(GenePairDataset(train_data), batch_size=args.train_batch_size, shuffle=True)\n",
    "                # Train\n",
    "                prediction_result_log_epoch=[]\n",
    "                label_log_epoch = []\n",
    "                for step,batch in enumerate(tqdm(gene_pair_loader)):\n",
    "                    node_a = batch[:, 0]\n",
    "                    node_b = batch[:, 1]\n",
    "                    node = torch.cat([node_a, node_b], dim=0)\n",
    "                    label = batch[:, 2].to(args.device_2)\n",
    "                    node_set = set(node_a.numpy()) | set(node_b.numpy())\n",
    "                    unique_node = list(node_set)\n",
    "                    len_unique_node = len(unique_node)\n",
    "                    node_mask = torch.zeros((27671)) # The number of gene/protein nodes in kg\n",
    "                    node_mask[unique_node] = 1\n",
    "                    node_mask = node_mask.bool()\n",
    "                    # find corresponding sequence data of node\n",
    "                    sequence_batch = []\n",
    "                    for one_node in node:\n",
    "                        sequence = sequence_data[sequence_data['Gene_id'] == one_node.item()]['Sequence'].values[0]\n",
    "                        # cut\n",
    "                        sequence = sequence[:args.esm_sequence_max_length]\n",
    "                        sequence_batch.append(sequence_data[sequence_data['Gene_id'] == one_node.item()]['Sequence'].values[0])\n",
    "                    \n",
    "                    kg_loader = HGTLoader(kgdata,\n",
    "                        num_samples={key: [args.sample_nodes] * args.sample_layers for key in kgdata.node_types},\n",
    "                        shuffle=False,\n",
    "                        batch_size=len_unique_node,\n",
    "                        input_nodes=(node_type,node_mask),\n",
    "                        num_workers=args.num_workers) # FIXME 这里效率瓶颈，每次都要重新构建loader，可以考虑提前构建好...吗？\n",
    "                    # get the whole batch of kg_loader\n",
    "                    \n",
    "                    \n",
    "                    # HGT forward\n",
    "                    for kg_batch in kg_loader:\n",
    "                        break\n",
    "                    kg_batch.to(args.device_0)\n",
    "                    node_rep= HGT_model(kg_batch.x_dict, kg_batch.edge_index_dict)\n",
    "                    node_rep=node_rep[node_type]\n",
    "                    node_set=pd.DataFrame(list(kg_batch[node_type].n_id[:len_unique_node].squeeze().detach().cpu().numpy()))\n",
    "                    node_set.drop_duplicates(inplace=True,keep='first')\n",
    "                    node_set[1]=range(node_set.shape[0])\n",
    "                    node_map=dict(zip(node_set[0],node_set[1]))\n",
    "                    # batch to pandas\n",
    "                    batch=pd.DataFrame(batch.numpy())\n",
    "                    # column name 0,1,2\n",
    "                    prediction_edge=batch[[0,1]]\n",
    "                    prediction_label=batch[2]\n",
    "                    edge_used.append(prediction_edge.shape[0])\n",
    "                    edge_a,edge_b=prediction_edge[0],prediction_edge[1]\n",
    "                    edge_a=edge_a.map(node_map)\n",
    "                    edge_b=edge_b.map(node_map)\n",
    "                    HGT_nodea_emb=node_rep[edge_a.values]\n",
    "                    HGT_nodeb_emb=node_rep[edge_b.values]\n",
    "                    \n",
    "                    # ESM forward\n",
    "                    # tokenize sequence\n",
    "                    if not args.use_esm_embedding:\n",
    "                        esm_input = ESM_tokenizer(sequence_batch,padding = True,truncation=True,return_tensors='pt')\n",
    "                        esm_input.to(args.device_3)\n",
    "                        sequence_batch_embedding = ESM_model(**esm_input).pooler_output\n",
    "                        ESM_nodea_emb = sequence_batch_embedding[:len(node_a)]\n",
    "                        ESM_nodeb_emb = sequence_batch_embedding[len(node_a):]\n",
    "                    else:\n",
    "                        ESM_nodea_emb = torch.stack([esm_embedding_geneid[one_node.item()] for one_node in node_a])\n",
    "                        ESM_nodeb_emb = torch.stack([esm_embedding_geneid[one_node.item()] for one_node in node_b])\n",
    "                        # do a linear forward to 320 dim\n",
    "                        ESM_nodea_emb = nn.Linear(1280, 320)(ESM_nodea_emb)\n",
    "                        ESM_nodeb_emb = nn.Linear(1280, 320)(ESM_nodeb_emb)\n",
    "                        \n",
    "                    \n",
    "                    cell_line_gene_data_nodea = []\n",
    "                    for one_node in node_a:\n",
    "                        # 使用loc来选择特定列并进行条件筛选\n",
    "                        selected_data = cell_line_gene_data.loc[cell_line_gene_data['Gene_id'] == one_node.item(), ['CN', 'Expression', 'HotspotMutation']]\n",
    "                        \n",
    "                        # 将选中的数据转换为列表并添加到结果列表中\n",
    "                        cell_line_gene_data_nodea.append(selected_data.values.tolist())\n",
    "                    cell_line_gene_data_nodea_embedding = torch.tensor(np.array(cell_line_gene_data_nodea).squeeze())\n",
    "                    cell_line_gene_data_nodeb = []\n",
    "                    for one_node in node_b:\n",
    "                        selected_data = cell_line_gene_data.loc[cell_line_gene_data['Gene_id'] == one_node.item(), ['CN', 'Expression', 'HotspotMutation']]\n",
    "                        cell_line_gene_data_nodeb.append(selected_data.values.tolist())\n",
    "                    cell_line_gene_data_nodeb_embedding = torch.tensor(np.array(cell_line_gene_data_nodeb).squeeze())\n",
    "                    HGT_nodea_emb = HGT_nodea_emb.to(args.device_2)\n",
    "                    HGT_nodeb_emb = HGT_nodeb_emb.to(args.device_2)                \n",
    "                    ESM_nodea_emb = ESM_nodea_emb.to(args.device_2)\n",
    "                    ESM_nodeb_emb = ESM_nodeb_emb.to(args.device_2)\n",
    "                    cell_line_gene_data_nodea_embedding = cell_line_gene_data_nodea_embedding.to(args.device_2)\n",
    "                    cell_line_gene_data_nodeb_embedding = cell_line_gene_data_nodeb_embedding.to(args.device_2)\n",
    "\n",
    "                    nodea_embedding = torch.cat([HGT_nodea_emb, ESM_nodea_emb, cell_line_gene_data_nodea_embedding], dim=1)\n",
    "                    nodeb_embedding = torch.cat([HGT_nodeb_emb, ESM_nodeb_emb, cell_line_gene_data_nodeb_embedding], dim=1)\n",
    "                    # embedding to float\n",
    "                    edge_embedding = torch.cat([nodea_embedding, nodeb_embedding], dim=1).float()\n",
    "\n",
    "                    embedding_dim = edge_embedding.shape[1]\n",
    "                    prediction_result = mlp(edge_embedding)\n",
    "                    prediction_result_log_epoch.append(prediction_result.detach().cpu().numpy())\n",
    "                    label_log_epoch.append(label.tolist())\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    criterion = nn.CrossEntropyLoss()\n",
    "                    loss = criterion(prediction_result, label)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                # prediction_result = prediction_result.flatten() \n",
    "                # prediction_result_log: (Step, Batch, 2) -> (Step * Batch, 2)\n",
    "                prediction_result_log_epoch = np.concatenate(prediction_result_log_epoch)\n",
    "                from itertools import chain\n",
    "                label_log_epoch_flat = np.array(list(chain.from_iterable(label_log_epoch)))\n",
    "                \n",
    "                aucu, aupr, f1, kappa, bacc = compute_accuracy(label_log_epoch_flat, np.array(prediction_result_log_epoch).argmax(axis=1), prediction_result_log_epoch)\n",
    "                auc_sum_fold.append(aucu)\n",
    "                aupr_sum_fold.append(aupr)\n",
    "                f1_sum_fold.append(f1)\n",
    "                bacc_sum_fold.append(bacc)\n",
    "                kappa_sum_fold.append(kappa)\n",
    "                logging.info(f\"Step {step}, Loss: {loss.item()}, AUC: {aucu}, AUPR: {aupr}, F1: {f1}, Kappa: {kappa}, BAcc: {bacc}\")\n",
    "                print(f\"Step {step}, Loss: {loss.item()}, AUC: {aucu}, AUPR: {aupr}, F1: {f1}, Kappa: {kappa}, BAcc: {bacc}\")\n",
    "            prediction_result_log_fold.append(prediction_result_log_epoch)\n",
    "            label_log_fold.append(label_log_epoch)\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_log_epoch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metagpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
