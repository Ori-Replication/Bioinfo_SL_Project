{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "from time import time\n",
    "import os\n",
    "import torch_geometric.transforms as T\n",
    "from MyLoader import HeteroDataset\n",
    "from torch_geometric.loader import HGTLoader, NeighborLoader\n",
    "# from dataloader import DataLoaderMasking \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from model import HGT\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "from torch_geometric.datasets import OGB_MAG\n",
    "import torch.nn.init as init\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score,auc,balanced_accuracy_score,cohen_kappa_score,precision_recall_curve\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, EsmModel\n",
    "import joblib\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args :\n",
    "    def __init__(self):\n",
    "        self.Full_data_path=r'../data/download_data/kgdata.pkl'\n",
    "        self.node_type='gene/protein'\n",
    "        self.Task_data_path = '../data/original_data/Cell_line_specific'\n",
    "        self.Save_model_path = '../logs_models/train_logs_models/'\n",
    "        self.processed_data_path = '../data/processed_data/'\n",
    "        self.cv = 'CV3'\n",
    "        self.n_fold = 5\n",
    "        self.device = 'cuda:1'\n",
    "        self.do_low_data = False\n",
    "        self.sample_nodes = 1024\n",
    "        self.sample_layers = 4\n",
    "        self.num_workers = 8\n",
    "        self.specific = True # Cell line specific\n",
    "        self.adapted = True # Cell line adapted\n",
    "        self.cell_line_list = ['A375','Jurkat','A549']\n",
    "        self.test_cell_line = 'A549'\n",
    "        self.freeze_graph_encoder = True\n",
    "        self.freeze_esm_encoder = True\n",
    "        self.folds = 5\n",
    "        self.do_train = True\n",
    "        self.train_batch_size = 128\n",
    "        self.test_batch_size = 128\n",
    "        self.hgt_emb_dim = 128\n",
    "        self.hgt_num_heads = 4\n",
    "        self.hgt_dropout_ratio = 0.2\n",
    "        self.hgt_num_layer = 3\n",
    "        self.mlp_hidden_dim = 128\n",
    "        self.lr = 1e-5\n",
    "        self.device_0 = 'cuda:0'\n",
    "        self.device_1 = 'cuda:1'\n",
    "        self.device_2 = 'cuda:2'\n",
    "        self.device_3 = 'cuda:3'\n",
    "        self.esm_sequence_max_length = 256\n",
    "        self.epoch = 100\n",
    "        self.use_esm_embedding = True\n",
    "        self.esm_embedding_file = '../data/download_data/gene_esm2emb.pkl'\n",
    "        \n",
    "args=args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_logger(args):\n",
    "    '''\n",
    "    Write logs to checkpoint and console \n",
    "    '''\n",
    "\n",
    "    if args.do_train:\n",
    "        # train_log=str(linear_layer_count)+'_'+args.lr+'_'+'train.log'\n",
    "        log_file = os.path.join(args.Save_model_path or args.init_checkpoint, 'train.log') \n",
    "    else:\n",
    "        log_file = os.path.join(args.Save_model_path or args.init_checkpoint, 'test.log') \n",
    "    \n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s %(levelname)-8s %(message)s', \n",
    "        level=logging.INFO,  # \n",
    "        datefmt='%Y-%m-%d %H:%M:%S', \n",
    "        filename=log_file, \n",
    "        filemode='w'  \n",
    "    )\n",
    "    console = logging.StreamHandler() # \n",
    "    console.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s %(levelname)-8s %(message)s') \n",
    "    console.setFormatter(formatter) \n",
    "    logging.getLogger('').addHandler(console) \n",
    "    \n",
    "def compute_accuracy(target, pred, pred_edge):\n",
    "    target = np.array(target)\n",
    "    pred = np.array(pred)\n",
    "    pred_edge = np.array(pred_edge)\n",
    "    \n",
    "    # 转换为 PyTorch 张量\n",
    "    pred_edge_tensor = torch.tensor(pred_edge, dtype=torch.float32)\n",
    "    scores = torch.softmax(pred_edge_tensor, dim=1).numpy()\n",
    "\n",
    "    \n",
    "    target = target.astype(int)\n",
    "    \n",
    "\n",
    "    \n",
    "    # 计算各项指标\n",
    "    aucu = roc_auc_score(target, scores[:, 1])\n",
    "    precision_tmp, recall_tmp, _thresholds = precision_recall_curve(target, scores[:, 1])\n",
    "    aupr = auc(recall_tmp, precision_tmp)\n",
    "    f1 = f1_score(target, pred)\n",
    "    kappa = cohen_kappa_score(target, pred)\n",
    "    bacc = balanced_accuracy_score(target, pred)\n",
    "    \n",
    "    return aucu, aupr, f1, kappa, bacc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_cell_line_gene_data(args, cell_line):\n",
    "    \"\"\"\n",
    "    load cell line specific gene data\n",
    "    \"\"\"\n",
    "    cell_line_gene_data = pd.read_csv(f\"{args.processed_data_path}/{cell_line}_all_data_gene.csv\")\n",
    "    return cell_line_gene_data\n",
    "\n",
    "def load_esm_embedding_data(args, node_index_data):\n",
    "    esm_embedding = joblib.load(args.esm_embedding_file )\n",
    "    esm_embedding_geneid = {}\n",
    "    for key, value in esm_embedding.items():\n",
    "        if key not in node_index_data['gene/protein']:\n",
    "            mapped_key = key  # Use original key or a placeholder if needed\n",
    "            esm_embedding_geneid[mapped_key] = torch.zeros(1280)\n",
    "        else:\n",
    "            mapped_key = node_index_data['gene/protein'][key]\n",
    "            esm_embedding_geneid[mapped_key] = value\n",
    "    return esm_embedding_geneid\n",
    "\n",
    "\n",
    "def Downstream_data_preprocess(args,n_fold,node_type_dict,cell_line): #FIXME\n",
    "    \"\"\"\n",
    "    load SL data and preprocess before training \n",
    "    \"\"\"\n",
    "    task_data_path=args.Task_data_path\n",
    "    train_data=pd.read_csv(f\"{task_data_path}/{cell_line}/sl_train_{n_fold}.csv\")\n",
    "    test_data=pd.read_csv(f\"{task_data_path}/{cell_line}/sl_test_{n_fold}.csv\",)\n",
    "    train_data.columns=[0,1,2,3]\n",
    "    test_data.columns=[0,1,2,3]\n",
    "    train_data[0]=train_data[0].astype(str).map(node_type_dict)\n",
    "    train_data[1]=train_data[1].astype(str).map(node_type_dict)\n",
    "    test_data[0]=test_data[0].astype(str).map(node_type_dict)\n",
    "    test_data[1]=test_data[1].astype(str).map(node_type_dict)\n",
    "    train_data=train_data.dropna()\n",
    "    test_data=test_data.dropna()\n",
    "    train_data[0]=train_data[0].astype(int)\n",
    "    train_data[1]=train_data[1].astype(int)\n",
    "    test_data[0]=test_data[0].astype(int)\n",
    "    test_data[1]=test_data[1].astype(int)\n",
    "    # low data scenario settings\n",
    "    if args.do_low_data:\n",
    "        num_sample=int(train_data.shape[0]*args.train_data_ratio)\n",
    "        print(num_sample)\n",
    "        train_data=train_data.sample(num_sample,replace=False,random_state=0)\n",
    "        train_data.reset_index(inplace=True)\n",
    "        print(f'train_data.size:{train_data.shape[0]}')\n",
    "\n",
    "    train_node=list(set(train_data[0])|set(train_data[1]))\n",
    "    print(f'train_node.size:{len(train_node)}')\n",
    "    train_mask=torch.zeros((27671))\n",
    "    test_mask=torch.zeros((27671))\n",
    "    test_node=list(set(test_data[0])|set(test_data[1]))\n",
    "    train_mask[train_node]=1\n",
    "    test_mask[test_node]=1\n",
    "    train_mask=train_mask.bool()\n",
    "    test_mask=test_mask.bool()\n",
    "    num_train_node=len(train_node)\n",
    "    num_test_node=len(test_node)\n",
    "    return train_data,test_data,train_mask,test_mask,num_train_node,num_test_node\n",
    "\n",
    "\n",
    "\n",
    "class GenePairDataset(Dataset):\n",
    "    def __init__(self, gene_pairs: pd.DataFrame):\n",
    "        # drop column 2\n",
    "        self.gene_pairs = gene_pairs.drop(columns=2).values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.gene_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.gene_pairs[idx]\n",
    "\n",
    "\n",
    "class sequence_dataset(Dataset):\n",
    "    def __init__(self,sequence_data):\n",
    "        self.sequence_data=sequence_data\n",
    "    def __len__(self):\n",
    "        return len(self.sequence_data)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.sequence_data[idx]\n",
    "\n",
    "def Construct_loader(args,kgdata,cell_line_gene_data,sequence_data,train_mask,test_mask,node_type,train_batch_size,test_batch_size):\n",
    "    \"\"\"\n",
    "    construct loader for train/test data\n",
    "    \"\"\"\n",
    "    \n",
    "    train_loader = HGTLoader(kgdata,\n",
    "    num_samples={key: [args.sample_nodes] * args.sample_layers for key in kgdata.node_types},shuffle=False,\n",
    "    batch_size=train_batch_size,\n",
    "    input_nodes=(node_type,train_mask),num_workers=args.num_workers)\n",
    "\n",
    "    return train_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "set_logger(args)\n",
    "with open (args.Full_data_path,'rb') as f:\n",
    "    kgdata=pickle.load(f)\n",
    "with open(\"../data/processed_data/gene_protein_2_id.json\",'rb') as f:\n",
    "    node_index=json.load(f)\n",
    "sequence_data = pd.read_csv('../data/train_data/uniprot_results_filtered.csv')\n",
    "sequence_data['Gene_id'] = sequence_data['Gene Name'].map(node_index['gene/protein'])\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "device = torch.device(args.device_0 )\n",
    "device_0 = torch.device(args.device_0)\n",
    "device_1 = torch.device(args.device_1)\n",
    "device_2 = torch.device(args.device_2)\n",
    "device_3 = torch.device(args.device_3)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "\n",
    "gene_protein=node_index[args.node_type] \n",
    "eval_metric_folds={'fold':[],'auc':[],'aupr':[],'f1':[],'bacc':[],'kappa':[]}\n",
    "node_type=args.node_type\n",
    "num_nodes_type=len(kgdata.node_types)\n",
    "num_edge_type=len(kgdata.edge_types)\n",
    "num_nodes=kgdata.num_nodes\n",
    "input_node_embeddings = torch.nn.Embedding(num_nodes_type, 16)\n",
    "torch.nn.init.xavier_uniform_(input_node_embeddings.weight.data)\n",
    "for i in range(len(kgdata.node_types)):\n",
    "    num_repeat=kgdata[kgdata.node_types[i]].x.shape[0]\n",
    "    kgdata[kgdata.node_types[i]].x =input_node_embeddings(torch.tensor(i)).repeat([num_repeat,1]).detach()\n",
    "\n",
    "HGT_model = HGT(kgdata,2*args.hgt_emb_dim,args.hgt_emb_dim,args.hgt_num_heads,args.hgt_num_layer).to(args.device_0)\n",
    "# HGT_model = nn.DataParallel(HGT_model, device_ids=[args.device_0, args.device_1, args.device_2, args.device_3]) \n",
    "# freeze\n",
    "# if args.freeze_graph_encoder:\n",
    "#     for param in HGT_model.parameters():\n",
    "#         param.requires_grad = False\n",
    "        \n",
    "\n",
    "if not args.use_esm_embedding:\n",
    "    ESM_tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "    ESM_model = EsmModel.from_pretrained(\"facebook/esm2_t6_8M_UR50D\").to(args.device_3)\n",
    "    ESM_model = nn.DataParallel(ESM_model, device_ids=[args.device_3, args.device_2, args.device_1])  # 使用两个设备\n",
    "    # freeze\n",
    "    if args.freeze_esm_encoder:\n",
    "        for param in ESM_model.parameters():\n",
    "            param.requires_grad = False\n",
    "else:\n",
    "    esm_embedding_geneid = load_esm_embedding_data(args, node_index)\n",
    "\n",
    "mlp = nn.Sequential(\n",
    "                nn.Linear(2*args.hgt_emb_dim+2*320+2*3, args.mlp_hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(args.mlp_hidden_dim, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32, 2),\n",
    "                ).to(args.device_2)\n",
    "\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=args.lr)\n",
    "\n",
    "if args.specific:# Do cv within cell line\n",
    "    logging.info(f\"Cell line specific\")\n",
    "    for cell_line in args.cell_line_list:\n",
    "        cell_line_gene_data = load_cell_line_gene_data(args, cell_line) \n",
    "        cell_line_gene_data['Gene_id'] = cell_line_gene_data['Gene Name'].map(node_index['gene/protein'])   \n",
    "        for fold in range(args.folds):\n",
    "            n_fold = fold\n",
    "            train_data,test_data,train_mask,test_mask,num_train_node,num_test_node=Downstream_data_preprocess(args,n_fold,gene_protein,cell_line)\n",
    "            loss_sum = 0\n",
    "            aucu_sum=0\n",
    "            f1_sum=0\n",
    "            bacc_sum=0\n",
    "            kappa_sum=0\n",
    "            aupr_sum=0\n",
    "            edge_used=[]\n",
    "             # map gene name(column name) to gene id\n",
    "            training_logs = []\n",
    "            testing_logs=[]\n",
    "            prediction_result_log_fold=[]\n",
    "            label_log_fold = []\n",
    "        \n",
    "            auc_sum_fold=[]\n",
    "            aupr_sum_fold=[]\n",
    "            f1_sum_fold=[]\n",
    "            bacc_sum_fold=[]\n",
    "            kappa_sum_fold=[]\n",
    "            for s in tqdm(range(args.epoch)):\n",
    "                gene_pair_loader = DataLoader(GenePairDataset(train_data), batch_size=args.train_batch_size, shuffle=True)\n",
    "                # Train\n",
    "                prediction_result_log_epoch=[]\n",
    "                label_log_epoch = []\n",
    "                for step,batch in enumerate(tqdm(gene_pair_loader)):\n",
    "                    node_a = batch[:, 0]\n",
    "                    node_b = batch[:, 1]\n",
    "                    node = torch.cat([node_a, node_b], dim=0)\n",
    "                    label = batch[:, 2].to(args.device_2)\n",
    "                    node_set = set(node_a.numpy()) | set(node_b.numpy())\n",
    "                    unique_node = list(node_set)\n",
    "                    len_unique_node = len(unique_node)\n",
    "                    node_mask = torch.zeros((27671)) # The number of gene/protein nodes in kg\n",
    "                    node_mask[unique_node] = 1\n",
    "                    node_mask = node_mask.bool()\n",
    "                    # find corresponding sequence data of node\n",
    "                    sequence_batch = []\n",
    "                    for one_node in node:\n",
    "                        sequence = sequence_data[sequence_data['Gene_id'] == one_node.item()]['Sequence'].values[0]\n",
    "                        # cut\n",
    "                        sequence = sequence[:args.esm_sequence_max_length]\n",
    "                        sequence_batch.append(sequence_data[sequence_data['Gene_id'] == one_node.item()]['Sequence'].values[0])\n",
    "                    \n",
    "                    kg_loader = HGTLoader(kgdata,\n",
    "                        num_samples={key: [args.sample_nodes] * args.sample_layers for key in kgdata.node_types},\n",
    "                        shuffle=False,\n",
    "                        batch_size=len_unique_node,\n",
    "                        input_nodes=(node_type,node_mask),\n",
    "                        num_workers=args.num_workers) # FIXME 这里效率瓶颈，每次都要重新构建loader，可以考虑提前构建好...吗？\n",
    "                    # get the whole batch of kg_loader\n",
    "                    \n",
    "                    \n",
    "                    # HGT forward\n",
    "                    for kg_batch in kg_loader:\n",
    "                        break\n",
    "                    kg_batch.to(args.device_0)\n",
    "                    node_rep= HGT_model(kg_batch.x_dict, kg_batch.edge_index_dict)\n",
    "                    node_rep=node_rep[node_type]\n",
    "                    node_set=pd.DataFrame(list(kg_batch[node_type].n_id[:len_unique_node].squeeze().detach().cpu().numpy()))\n",
    "                    node_set.drop_duplicates(inplace=True,keep='first')\n",
    "                    node_set[1]=range(node_set.shape[0])\n",
    "                    node_map=dict(zip(node_set[0],node_set[1]))\n",
    "                    # batch to pandas\n",
    "                    batch=pd.DataFrame(batch.numpy())\n",
    "                    # column name 0,1,2\n",
    "                    prediction_edge=batch[[0,1]]\n",
    "                    prediction_label=batch[2]\n",
    "                    edge_used.append(prediction_edge.shape[0])\n",
    "                    edge_a,edge_b=prediction_edge[0],prediction_edge[1]\n",
    "                    edge_a=edge_a.map(node_map)\n",
    "                    edge_b=edge_b.map(node_map)\n",
    "                    HGT_nodea_emb=node_rep[edge_a.values]\n",
    "                    HGT_nodeb_emb=node_rep[edge_b.values]\n",
    "                    \n",
    "                    # ESM forward\n",
    "                    # tokenize sequence\n",
    "                    if not args.use_esm_embedding:\n",
    "                        esm_input = ESM_tokenizer(sequence_batch,padding = True,truncation=True,return_tensors='pt')\n",
    "                        esm_input.to(args.device_3)\n",
    "                        sequence_batch_embedding = ESM_model(**esm_input).pooler_output\n",
    "                        ESM_nodea_emb = sequence_batch_embedding[:len(node_a)]\n",
    "                        ESM_nodeb_emb = sequence_batch_embedding[len(node_a):]\n",
    "                    else:\n",
    "                        ESM_nodea_emb = torch.stack([esm_embedding_geneid[one_node.item()] for one_node in node_a])\n",
    "                        ESM_nodeb_emb = torch.stack([esm_embedding_geneid[one_node.item()] for one_node in node_b])\n",
    "                        # do a linear forward to 320 dim\n",
    "                        ESM_nodea_emb = nn.Linear(1280, 320)(ESM_nodea_emb)\n",
    "                        ESM_nodeb_emb = nn.Linear(1280, 320)(ESM_nodeb_emb)\n",
    "                        \n",
    "                    \n",
    "                    cell_line_gene_data_nodea = []\n",
    "                    for one_node in node_a:\n",
    "                        selected_data = cell_line_gene_data.loc[cell_line_gene_data['Gene_id'] == one_node.item(), ['CN', 'Expression', 'HotspotMutation']]\n",
    "                        cell_line_gene_data_nodea.append(selected_data.values.tolist())\n",
    "                    cell_line_gene_data_nodea_embedding = torch.tensor(np.array(cell_line_gene_data_nodea).squeeze())\n",
    "                    cell_line_gene_data_nodeb = []\n",
    "                    for one_node in node_b:\n",
    "                        selected_data = cell_line_gene_data.loc[cell_line_gene_data['Gene_id'] == one_node.item(), ['CN', 'Expression', 'HotspotMutation']]\n",
    "                        cell_line_gene_data_nodeb.append(selected_data.values.tolist())\n",
    "                    cell_line_gene_data_nodeb_embedding = torch.tensor(np.array(cell_line_gene_data_nodeb).squeeze())\n",
    "                    \n",
    "                    \n",
    "                    HGT_nodea_emb = HGT_nodea_emb.to(args.device_2)\n",
    "                    HGT_nodeb_emb = HGT_nodeb_emb.to(args.device_2)                \n",
    "                    ESM_nodea_emb = ESM_nodea_emb.to(args.device_2)\n",
    "                    ESM_nodeb_emb = ESM_nodeb_emb.to(args.device_2)\n",
    "                    cell_line_gene_data_nodea_embedding = cell_line_gene_data_nodea_embedding.to(args.device_2)\n",
    "                    cell_line_gene_data_nodeb_embedding = cell_line_gene_data_nodeb_embedding.to(args.device_2)\n",
    "\n",
    "                    nodea_embedding = torch.cat([HGT_nodea_emb, ESM_nodea_emb, cell_line_gene_data_nodea_embedding], dim=1)\n",
    "                    nodeb_embedding = torch.cat([HGT_nodeb_emb, ESM_nodeb_emb, cell_line_gene_data_nodeb_embedding], dim=1)\n",
    "                    # embedding to float\n",
    "                    edge_embedding = torch.cat([nodea_embedding, nodeb_embedding], dim=1).float()\n",
    "\n",
    "                    embedding_dim = edge_embedding.shape[1]\n",
    "                    prediction_result = mlp(edge_embedding)\n",
    "                    prediction_result_log_epoch.append(prediction_result.detach().cpu().numpy())\n",
    "                    label_log_epoch.append(label.tolist())\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    criterion = nn.CrossEntropyLoss()\n",
    "                    loss = criterion(prediction_result, label)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                # prediction_result = prediction_result.flatten() \n",
    "                # prediction_result_log: (Step, Batch, 2) -> (Step * Batch, 2)\n",
    "                prediction_result_log_epoch = np.concatenate(prediction_result_log_epoch)\n",
    "                from itertools import chain\n",
    "                label_log_epoch_flat = np.array(list(chain.from_iterable(label_log_epoch)))\n",
    "                \n",
    "                aucu, aupr, f1, kappa, bacc = compute_accuracy(label_log_epoch_flat, np.array(prediction_result_log_epoch).argmax(axis=1), prediction_result_log_epoch)\n",
    "                auc_sum_fold.append(aucu)\n",
    "                aupr_sum_fold.append(aupr)\n",
    "                f1_sum_fold.append(f1)\n",
    "                bacc_sum_fold.append(bacc)\n",
    "                kappa_sum_fold.append(kappa)\n",
    "                logging.info(f\"Step {step}, Loss: {loss.item()}, AUC: {aucu}, AUPR: {aupr}, F1: {f1}, Kappa: {kappa}, BAcc: {bacc}\")\n",
    "                print(f\"Step {step}, Loss: {loss.item()}, AUC: {aucu}, AUPR: {aupr}, F1: {f1}, Kappa: {kappa}, BAcc: {bacc}\")\n",
    "            prediction_result_log_fold.append(prediction_result_log_epoch)\n",
    "            label_log_fold.append(label_log_epoch)\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在第5步时，没有集齐四张有奖卡牌的概率为: 0.1317\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def probability_not_collecting_four_prize_cards(k):\n",
    "    # 定义转移矩阵\n",
    "    P = np.zeros((5, 5))  # 5种状态，0-4张有奖卡牌\n",
    "\n",
    "    # 填充转移矩阵\n",
    "    for x in range(5):  # 当前有奖卡牌数\n",
    "        if x < 4:  # 还没集齐4张有奖卡牌\n",
    "            P[x, x + 1] = (4 - x) / (12 - x)  # 抽到有奖卡牌\n",
    "            P[x, x] = 1 - P[x, x + 1]  # 抽到无奖卡牌\n",
    "        else:  # 一旦集齐4张有奖卡牌，就吸收\n",
    "            P[x, x] = 1.0\n",
    "\n",
    "    # 初始状态概率分布，只有0张有奖卡牌的概率为1，其余为0\n",
    "    p = np.zeros(5)\n",
    "    p[0] = 1.0\n",
    "\n",
    "    # 在第k步的状态概率分布\n",
    "    p_k = np.linalg.matrix_power(P, k).dot(p)\n",
    "\n",
    "    # 没有集齐4张有奖卡牌的概率\n",
    "    return sum(p_k[:-1])  # 排除最后一个吸收状态\n",
    "\n",
    "# 示例：计算第k步时没有集齐四张有奖卡牌的概率\n",
    "k = 5\n",
    "prob = probability_not_collecting_four_prize_cards(k)\n",
    "print(f\"在第{k}步时，没有集齐四张有奖卡牌的概率为: {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metagpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
