{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "from time import time\n",
    "import os\n",
    "import torch_geometric.transforms as T\n",
    "from MyLoader import HeteroDataset\n",
    "from torch_geometric.loader import HGTLoader, NeighborLoader\n",
    "# from dataloader import DataLoaderMasking \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from model import HGT\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "from torch_geometric.datasets import OGB_MAG\n",
    "import torch.nn.init as init\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score,auc,balanced_accuracy_score,cohen_kappa_score,precision_recall_curve, average_precision_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, EsmModel\n",
    "import joblib\n",
    "import torch_sparse\n",
    "from itertools import chain\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args :\n",
    "    def __init__(self):\n",
    "        self.Full_data_path=r'../data/download_data/kgdata.pkl'\n",
    "        self.node_type='gene/protein'\n",
    "        self.Task_data_path = '../data/train_data/Cell_line_specific'\n",
    "        self.Save_model_path = '../logs_models/train_logs_models/'\n",
    "        self.processed_data_path = '../data/processed_data/'\n",
    "        self.init_checkpoint = '../logs_models/pretrained_models/Primekg_HGT_0.2_0.001'\n",
    "        self.cv = 'CV3'\n",
    "        self.n_fold = 5\n",
    "        self.do_low_data = False\n",
    "        self.sample_nodes = 1024\n",
    "        self.sample_layers = 4\n",
    "        self.num_workers = 8\n",
    "        self.specific = True # Cell line specific\n",
    "        self.adapted = True # Cell line adapted\n",
    "        self.cell_line_list = ['A549']\n",
    "        self.test_cell_line = 'A549'\n",
    "        self.freeze_graph_encoder = True\n",
    "        self.freeze_esm_encoder = True\n",
    "        self.folds = 5\n",
    "        self.do_train = True\n",
    "        self.train_batch_size = 256\n",
    "        self.test_batch_size = 256\n",
    "        self.hgt_emb_dim = 128\n",
    "        self.hgt_num_heads = 4\n",
    "        self.hgt_dropout_ratio = 0.2\n",
    "        self.hgt_num_layer = 4\n",
    "        self.mlp_hidden_dim = 128\n",
    "        self.lr = 1e-5\n",
    "        self.device = 'cuda:2'\n",
    "        self.device_0 = 'cuda:0'\n",
    "        self.device_1 = 'cuda:1'\n",
    "        self.device_2 = 'cuda:2'\n",
    "        self.device_3 = 'cuda:3'\n",
    "        self.esm_sequence_max_length = 256\n",
    "        self.epoch = 30\n",
    "        self.use_esm_embedding = True\n",
    "        self.esm_embedding_file = '../data/download_data/gene_esm2emb.pkl'\n",
    "        self.decay = 0\n",
    "        \n",
    "        \n",
    "args=args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_logger(args):\n",
    "    '''\n",
    "    Write logs to checkpoint and console \n",
    "    '''\n",
    "\n",
    "    if args.do_train:\n",
    "        # train_log=str(linear_layer_count)+'_'+args.lr+'_'+'train.log'\n",
    "        log_file = os.path.join(args.Save_model_path or args.init_checkpoint, 'train.log') \n",
    "    else:\n",
    "        log_file = os.path.join(args.Save_model_path or args.init_checkpoint, 'test.log') \n",
    "    \n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s %(levelname)-8s %(message)s', \n",
    "        level=logging.INFO,  # \n",
    "        datefmt='%Y-%m-%d %H:%M:%S', \n",
    "        filename=log_file, \n",
    "        filemode='w'  \n",
    "    )\n",
    "    console = logging.StreamHandler() # \n",
    "    console.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s %(levelname)-8s %(message)s') \n",
    "    console.setFormatter(formatter) \n",
    "    logging.getLogger('').addHandler(console) \n",
    "    \n",
    "def compute_accuracy(target, pred, pred_edge):\n",
    "    target = np.array(target)\n",
    "    pred = np.array(pred)\n",
    "    pred_edge = np.array(pred_edge)\n",
    "    \n",
    "    # 转换为 PyTorch 张量\n",
    "    pred_edge_tensor = torch.tensor(pred_edge, dtype=torch.float32)\n",
    "    scores = torch.softmax(pred_edge_tensor, dim=1).numpy()\n",
    "\n",
    "    \n",
    "    target = target.astype(int)\n",
    "    \n",
    "\n",
    "    \n",
    "    # 计算各项指标\n",
    "    aucu = roc_auc_score(target, scores[:, 1])\n",
    "    precision_tmp, recall_tmp, _thresholds = precision_recall_curve(target, scores[:, 1])\n",
    "    aupr = auc(recall_tmp, precision_tmp)\n",
    "    aupr= average_precision_score(target,scores[:,1])\n",
    "    f1 = f1_score(target, pred)\n",
    "    kappa = cohen_kappa_score(target, pred)\n",
    "    bacc = balanced_accuracy_score(target, pred)\n",
    "    \n",
    "    return aucu, aupr, f1, kappa, bacc\n",
    "\n",
    "\n",
    "def compute_accuracy_2(target,pred, pred_edge):\n",
    "    \n",
    "    target=target.clone().detach().cpu().numpy()\n",
    "    pred=pred.clone().detach().cpu().numpy()\n",
    "    pred_edge=pred_edge.clone().detach().cpu()\n",
    "    scores = torch.softmax(pred_edge, 1).numpy()\n",
    "    target=target.astype(int)\n",
    "    \n",
    "    print(target)\n",
    "    print(scores[:,1])\n",
    "   \n",
    "    aucu=roc_auc_score(target,scores[:,1])\n",
    "    precision_tmp, recall_tmp, _thresholds = precision_recall_curve(target, pred)\n",
    "    aupr = auc(recall_tmp, precision_tmp)\n",
    "    aupr= average_precision_score(target,scores[:,1])\n",
    "    f1 = f1_score(target,pred)\n",
    "    kappa=cohen_kappa_score(target,pred)\n",
    "    bacc=balanced_accuracy_score(target,pred)\n",
    "    \n",
    "    return aucu,aupr,f1,kappa,bacc\n",
    "\n",
    "\n",
    "def load_cell_line_gene_data(args, cell_line):\n",
    "    \"\"\"\n",
    "    load cell line specific gene data\n",
    "    \"\"\"\n",
    "    cell_line_gene_data = pd.read_csv(f\"{args.processed_data_path}/{cell_line}_all_data_gene.csv\")\n",
    "    return cell_line_gene_data\n",
    "\n",
    "\n",
    "\n",
    "def load_esm_embedding_data(args, node_index_data):\n",
    "    esm_embedding = joblib.load(args.esm_embedding_file )\n",
    "    esm_embedding_geneid = {}\n",
    "    for key, value in esm_embedding.items():\n",
    "        if key not in node_index_data['gene/protein']:\n",
    "            mapped_key = key  # Use original key or a placeholder if needed\n",
    "            esm_embedding_geneid[mapped_key] = torch.zeros(1280)\n",
    "        else:\n",
    "            mapped_key = node_index_data['gene/protein'][key]\n",
    "            esm_embedding_geneid[mapped_key] = value\n",
    "    return esm_embedding_geneid\n",
    "\n",
    "\n",
    "def Downstream_data_preprocess(args,n_fold,node_type_dict,cell_line): #FIXME\n",
    "    \"\"\"\n",
    "    load SL data and preprocess before training \n",
    "    \"\"\"\n",
    "    task_data_path=args.Task_data_path\n",
    "    train_data=pd.read_csv(f\"{task_data_path}/{cell_line}/train_{n_fold}.csv\")\n",
    "    test_data=pd.read_csv(f\"{task_data_path}/{cell_line}/valid_{n_fold}.csv\",)\n",
    "    train_data.columns=[0,1,2,3]\n",
    "    test_data.columns=[0,1,2,3]\n",
    "    train_data[0]=train_data[0].astype(str).map(node_type_dict)\n",
    "    train_data[1]=train_data[1].astype(str).map(node_type_dict)\n",
    "    test_data[0]=test_data[0].astype(str).map(node_type_dict)\n",
    "    test_data[1]=test_data[1].astype(str).map(node_type_dict)\n",
    "    train_data=train_data.dropna()\n",
    "    test_data=test_data.dropna()\n",
    "    train_data[0]=train_data[0].astype(int)\n",
    "    train_data[1]=train_data[1].astype(int)\n",
    "    test_data[0]=test_data[0].astype(int)\n",
    "    test_data[1]=test_data[1].astype(int)\n",
    "    # low data scenario settings\n",
    "    if args.do_low_data:\n",
    "        num_sample=int(train_data.shape[0]*args.train_data_ratio)\n",
    "        print(num_sample)\n",
    "        train_data=train_data.sample(num_sample,replace=False,random_state=0)\n",
    "        train_data.reset_index(inplace=True)\n",
    "        print(f'train_data.size:{train_data.shape[0]}')\n",
    "\n",
    "    train_node=list(set(train_data[0])|set(train_data[1]))\n",
    "    print(f'train_node.size:{len(train_node)}')\n",
    "    train_mask=torch.zeros((27671))\n",
    "    test_mask=torch.zeros((27671))\n",
    "    test_node=list(set(test_data[0])|set(test_data[1]))\n",
    "    train_mask[train_node]=1\n",
    "    test_mask[test_node]=1\n",
    "    train_mask=train_mask.bool()\n",
    "    test_mask=test_mask.bool()\n",
    "    num_train_node=len(train_node)\n",
    "    num_test_node=len(test_node)\n",
    "    return train_data,test_data,train_mask,test_mask,num_train_node,num_test_node\n",
    "\n",
    "def override_config(args):\n",
    "    '''\n",
    "    Override model and data configuration \n",
    "    '''\n",
    "    with open(os.path.join(args.init_checkpoint, 'config.json'), 'r') as fjson:\n",
    "        argparse_dict = json.load(fjson)\n",
    "    \n",
    "    args.method=argparse_dict['method']\n",
    "    # args.epochs = argparse_dict['epochs']\n",
    "    args.lr = argparse_dict['lr']\n",
    "    args.num_layer = argparse_dict['num_layer']\n",
    "    args.emb_dim = argparse_dict['emb_dim']\n",
    "    args.mask_rate = argparse_dict['mask_rate']\n",
    "    args.gnn_type=argparse_dict['gnn_type']\n",
    "\n",
    "    if args.Save_model_path is None:\n",
    "        args.Save_model_path = argparse_dict['Save_model_path']\n",
    "\n",
    "class GenePairDataset(Dataset):\n",
    "    def __init__(self, gene_pairs: pd.DataFrame):\n",
    "        # drop column 2\n",
    "        self.gene_pairs = gene_pairs.drop(columns=2).values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.gene_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.gene_pairs[idx]\n",
    "\n",
    "\n",
    "class sequence_dataset(Dataset):\n",
    "    def __init__(self,sequence_data):\n",
    "        self.sequence_data=sequence_data\n",
    "    def __len__(self):\n",
    "        return len(self.sequence_data)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.sequence_data[idx]\n",
    "\n",
    "def Construct_loader(args,kgdata,cell_line_gene_data,sequence_data,train_mask,test_mask,node_type,train_batch_size,test_batch_size):\n",
    "    \"\"\"\n",
    "    construct loader for train/test data\n",
    "    \"\"\"\n",
    "    \n",
    "    train_loader = HGTLoader(kgdata,\n",
    "    num_samples={key: [args.sample_nodes] * args.sample_layers for key in kgdata.node_types},shuffle=False,\n",
    "    batch_size=train_batch_size,\n",
    "    input_nodes=(node_type,train_mask),num_workers=args.num_workers)\n",
    "\n",
    "    return train_loader\n",
    "\n",
    "\n",
    "\n",
    "# HGT Model with a classification head\n",
    "class HGT4Classification(nn.Module):\n",
    "    def __init__(self,args, hgt,emb_dim,hidden_dim, num_classes, len_unique_node):\n",
    "        super(HGT4Classification, self).__init__()\n",
    "        self.hgt = hgt\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "        self.args = args\n",
    "    \n",
    "    def forward(self, kg_batch,batch):\n",
    "        node_rep = self.hgt(kg_batch.x_dict, kg_batch.edge_index_dict)\n",
    "        node_rep=   node_rep[args.node_type]\n",
    "        node_set=pd.DataFrame(list(kg_batch[node_type].n_id[:len_unique_node].squeeze().detach().cpu().numpy()))\n",
    "        node_set.drop_duplicates(inplace=True,keep='first')\n",
    "        node_set[1]=range(node_set.shape[0])\n",
    "        node_map=dict(zip(node_set[0],node_set[1]))\n",
    "        batch=pd.DataFrame(batch.numpy())\n",
    "        prediction_edge=batch[[0,1]]\n",
    "        prediction_label=batch[2]\n",
    "        edge_a,edge_b=prediction_edge[0],prediction_edge[1]\n",
    "        edge_a=edge_a.map(node_map)\n",
    "        edge_b=edge_b.map(node_map)\n",
    "        HGT_nodea_emb=node_rep[edge_a.values]\n",
    "        HGT_nodeb_emb=node_rep[edge_b.values]\n",
    "        edge_embedding = torch.cat([HGT_nodea_emb, HGT_nodeb_emb], dim=1)\n",
    "        emb_dim = edge_embedding.size(1)\n",
    "        pred = self.mlp(edge_embedding)\n",
    "        return pred\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "set_logger(args)\n",
    "with open (args.Full_data_path,'rb') as f:\n",
    "    kgdata=pickle.load(f)\n",
    "with open(\"../data/processed_data/gene_protein_2_id.json\",'rb') as f:\n",
    "    node_index=json.load(f)\n",
    "sequence_data = pd.read_csv('../data/train_data/uniprot_results_filtered.csv')\n",
    "sequence_data['Gene_id'] = sequence_data['Gene Name'].map(node_index['gene/protein'])\n",
    "if args.init_checkpoint:  \n",
    "    override_config(args)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "device = torch.device(args.device_1 )\n",
    "device_0 = torch.device(args.device_0)\n",
    "device_1 = torch.device(args.device_1)\n",
    "device_2 = torch.device(args.device_2)\n",
    "device_3 = torch.device(args.device_3)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    \n",
    "gene_protein=node_index[args.node_type] \n",
    "eval_metric_folds={'fold':[],'auc':[],'aupr':[],'f1':[],'bacc':[],'kappa':[]}\n",
    "node_type=args.node_type\n",
    "num_nodes_type=len(kgdata.node_types)\n",
    "num_edge_type=len(kgdata.edge_types)\n",
    "num_nodes=kgdata.num_nodes\n",
    "input_node_embeddings = torch.nn.Embedding(num_nodes_type, 16)\n",
    "torch.nn.init.xavier_uniform_(input_node_embeddings.weight.data)\n",
    "for i in range(len(kgdata.node_types)):\n",
    "    num_repeat=kgdata[kgdata.node_types[i]].x.shape[0]\n",
    "    kgdata[kgdata.node_types[i]].x =input_node_embeddings(torch.tensor(i)).repeat([num_repeat,1]).detach()\n",
    "\n",
    "HGT_model = HGT(kgdata,2*args.hgt_emb_dim,args.hgt_emb_dim,args.hgt_num_heads,args.hgt_num_layer).to(args.device)\n",
    "\n",
    "if args.init_checkpoint: \n",
    "    # Restore model from checkpoint directory  \n",
    "    logging.info('Loading checkpoint %s...' % args.init_checkpoint)\n",
    "    checkpoint = torch.load(os.path.join(args.init_checkpoint, 'checkpoint'))\n",
    "    init_step = checkpoint['step']\n",
    "    HGT_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "HGT4Classification_model = HGT4Classification(args, HGT_model, args.hgt_emb_dim, 32, 2, args.train_batch_size).to(args.device)\n",
    "# HGT4Classification_model = nn.DataParallel(HGT4Classification_model, device_ids=[0, 1, 2, 3])\n",
    "optimizer_model = optim.Adam(HGT4Classification_model.parameters(), lr=args.lr, weight_decay=args.decay)\n",
    "\n",
    "logging.info(f\"Cell line specific\")\n",
    "for cell_line in args.cell_line_list:\n",
    "    cell_line_gene_data = load_cell_line_gene_data(args, cell_line) \n",
    "    cell_line_gene_data['Gene_id'] = cell_line_gene_data['Gene Name'].map(node_index['gene/protein'])   \n",
    "    for fold in range(args.folds):\n",
    "        n_fold = fold\n",
    "        train_data,test_data,train_mask,test_mask,num_train_node,num_test_node=Downstream_data_preprocess(args,n_fold,gene_protein,cell_line)\n",
    "        loss_sum = 0\n",
    "        aucu_sum=0\n",
    "        f1_sum=0\n",
    "        bacc_sum=0\n",
    "        kappa_sum=0\n",
    "        aupr_sum=0\n",
    "        edge_used=[]\n",
    "            # map gene name(column name) to gene id\n",
    "        training_logs = []\n",
    "        testing_logs=[]\n",
    "        prediction_result_log_fold=[]\n",
    "        label_log_fold = []\n",
    "    \n",
    "        auc_sum_fold=[]\n",
    "        aupr_sum_fold=[]\n",
    "        f1_sum_fold=[]\n",
    "        bacc_sum_fold=[]\n",
    "        kappa_sum_fold=[]\n",
    "        for epoch in tqdm(range(args.epoch)):\n",
    "            HGT4Classification_model.train()\n",
    "            gene_pair_loader = DataLoader(GenePairDataset(train_data), batch_size=args.train_batch_size, shuffle=True)\n",
    "            # Train\n",
    "            prediction_result_log_epoch=[]\n",
    "            label_log_epoch = []\n",
    "            loss_sum = 0\n",
    "            for step,batch in enumerate(tqdm(gene_pair_loader)):\n",
    "                optimizer_model.zero_grad()\n",
    "                node_a = batch[:, 0]\n",
    "                node_b = batch[:, 1]\n",
    "                node = torch.cat([node_a, node_b], dim=0)\n",
    "                label = batch[:, 2].to(args.device)\n",
    "                node_set = set(node_a.numpy()) | set(node_b.numpy())\n",
    "                unique_node = list(node_set)\n",
    "                len_unique_node = len(unique_node)\n",
    "                node_mask = torch.zeros((27671)) # The number of gene/protein nodes in kg\n",
    "                node_mask[unique_node] = 1\n",
    "                node_mask = node_mask.bool()\n",
    "                \n",
    "                kg_loader = HGTLoader(kgdata,\n",
    "                    num_samples={key: [args.sample_nodes] * args.sample_layers for key in kgdata.node_types},\n",
    "                    shuffle=False,\n",
    "                    batch_size=len_unique_node,\n",
    "                    input_nodes=(node_type,node_mask),\n",
    "                    num_workers=args.num_workers) \n",
    "                for kg_batch in kg_loader:\n",
    "                    break\n",
    "                kg_batch.to(args.device)\n",
    "                prediction_result = HGT4Classification_model(kg_batch,batch)\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                loss = criterion(prediction_result, label)\n",
    "                loss.backward()\n",
    "                loss_sum += loss.item()\n",
    "                torch.nn.utils.clip_grad_norm_(HGT4Classification_model.parameters(), max_norm=1.0)\n",
    "                optimizer_model.step()\n",
    "            # print(f\"Epoch {s} loss: {loss_sum}\")\n",
    "            \n",
    "            # valid\n",
    "            HGT4Classification_model.eval()\n",
    "            gene_pair_loader = DataLoader(GenePairDataset(test_data), batch_size=args.test_batch_size, shuffle=False)\n",
    "            aucu_sum = 0\n",
    "            f1_sum=0\n",
    "            bacc_sum=0\n",
    "            kappa_sum=0\n",
    "            aupr_sum=0\n",
    "            edge_used=[]\n",
    "            with torch.no_grad():\n",
    "                for step,batch in enumerate(tqdm(gene_pair_loader)):\n",
    "                    node_a = batch[:, 0]\n",
    "                    node_b = batch[:, 1]\n",
    "                    node = torch.cat([node_a, node_b], dim=0)\n",
    "                    label = batch[:, 2].to(args.device)\n",
    "                    node_set = set(node_a.numpy()) | set(node_b.numpy())\n",
    "                    unique_node = list(node_set)\n",
    "                    len_unique_node = len(unique_node)\n",
    "                    node_mask = torch.zeros((27671))\n",
    "                    node_mask[unique_node] = 1\n",
    "                    node_mask = node_mask.bool()\n",
    "                    \n",
    "                    kg_loader = HGTLoader(kgdata,\n",
    "                        num_samples={key: [args.sample_nodes] * args.sample_layers for key in kgdata.node_types},\n",
    "                        shuffle=False,\n",
    "                        batch_size=len_unique_node,\n",
    "                        input_nodes=(node_type,node_mask),\n",
    "                        num_workers=args.num_workers)\n",
    "                    \n",
    "                    for kg_batch in kg_loader:\n",
    "                        break\n",
    "                    kg_batch.to(args.device)\n",
    "                    prediction_result = HGT4Classification_model(kg_batch,batch)\n",
    "                    prediction_result_log_epoch.append(prediction_result.detach().cpu().numpy())\n",
    "                    label_log_epoch.append(label.tolist())\n",
    "            prediction_result_log_epoch = np.concatenate(prediction_result_log_epoch)\n",
    "            label_log_epoch_flat = np.array(list(chain.from_iterable(label_log_epoch)))\n",
    "            \n",
    "\n",
    "            aucu, aupr, f1, kappa, bacc = compute_accuracy(label_log_epoch_flat, np.array(prediction_result_log_epoch).argmax(axis=1), prediction_result_log_epoch)\n",
    "            auc_sum_fold.append(aucu)\n",
    "            aupr_sum_fold.append(aupr)\n",
    "            f1_sum_fold.append(f1)\n",
    "            bacc_sum_fold.append(bacc)\n",
    "            kappa_sum_fold.append(kappa)\n",
    "            print(f\"Epoch {epoch}, Loss: {loss_sum}, AUC: {aucu}, AUPR: {aupr}, F1: {f1}, Kappa: {kappa}, BAcc: {bacc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法启动 Kernel。 \n",
      "\u001b[1;31m内核已终止。错误: ...有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# 初始化参数\n",
    "N = 12  # 总牌数\n",
    "K = 3   # 有奖牌数\n",
    "r = 3   # 目标是找到第3张有奖牌\n",
    "\n",
    "# 计算期望\n",
    "E = 0   # 初始期望\n",
    "p_miss = 1 # 未中奖概率\n",
    "for i in range(r, N+1):\n",
    "    p = (math.comb(K,r) * math.comb(N-K,i-r))/(math.comb(N,i))\n",
    "    q = 1 - p\n",
    "    p_miss *= q\n",
    "    print(\"i:\",i,\"P(K=r)累乘:\",1-p_miss)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model loading\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioinfo_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
