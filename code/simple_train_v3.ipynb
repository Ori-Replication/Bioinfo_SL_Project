{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "from time import time\n",
    "import os\n",
    "import torch_geometric.transforms as T\n",
    "from MyLoader import HeteroDataset\n",
    "from torch_geometric.loader import HGTLoader, NeighborLoader\n",
    "# from dataloader import DataLoaderMasking \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from model import HGT\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "from torch_geometric.datasets import OGB_MAG\n",
    "import torch.nn.init as init\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score,auc,balanced_accuracy_score,cohen_kappa_score,precision_recall_curve, average_precision_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, EsmModel\n",
    "import joblib\n",
    "import torch_sparse\n",
    "from itertools import chain\n",
    "import datetime\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args :\n",
    "    def __init__(self):\n",
    "        self.Full_data_path=r'../data/download_data/kgdata.pkl'\n",
    "        self.node_type='gene/protein'\n",
    "        self.Task_data_path = '../data/train_data/Cell_line_specific'\n",
    "        self.Save_model_path = '../logs_models/train_logs_models/'\n",
    "        self.processed_data_path = '../data/processed_data/'\n",
    "        self.init_checkpoint = '../logs_models/pretrained_models/Primekg_HGT_0.2_0.001'\n",
    "        self.cv = 'CV3'\n",
    "        self.n_fold = 3\n",
    "        self.do_low_data = False\n",
    "        self.sample_nodes = 1024\n",
    "        self.sample_layers = 4\n",
    "        self.num_workers = 8\n",
    "        self.specific = True # Cell line specific\n",
    "        self.adapted = True # Cell line adapted\n",
    "        self.cell_line_list = ['A549']\n",
    "        self.test_cell_line = 'A549'\n",
    "        self.freeze_graph_encoder = True\n",
    "        self.freeze_esm_encoder = True\n",
    "        self.folds = 5\n",
    "        self.do_train = True\n",
    "        self.train_batch_size = 512\n",
    "        self.test_batch_size = 512\n",
    "        self.hgt_emb_dim = 128\n",
    "        self.hgt_num_heads = 4\n",
    "        self.hgt_dropout_ratio = 0.2\n",
    "        self.hgt_num_layer = 4\n",
    "        self.mlp_hidden_dim = 256\n",
    "        self.lr = 1e-5\n",
    "        self.device = 'cuda:2'\n",
    "        self.device_0 = 'cuda:0'\n",
    "        self.device_1 = 'cuda:1'\n",
    "        self.device_2 = 'cuda:2'\n",
    "        self.device_3 = 'cuda:3'\n",
    "        self.esm_sequence_max_length = 256\n",
    "        self.epoch = 20\n",
    "        self.use_esm_embedding = True\n",
    "        self.esm_embedding_file = '../data/download_data/gene_esm2emb.pkl'\n",
    "        self.decay = 1e-6\n",
    "        self.attention_classifier_num_heads = 4\n",
    "        self.weight_decay = 1e-5\n",
    "        self.esm_reduction_dim = 256\n",
    "        self.hgt_lr = 1e-5\n",
    "        self.fc_lr = 1e-4\n",
    "        self.fc_weight_decay = 1e-5\n",
    "        self.base_weight_decay = 0\n",
    "        \n",
    "        \n",
    "args=args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_log_dir(args):\n",
    "    \"\"\"\n",
    "    Generate a directory name based on the current time and cell line names in args.\n",
    "    \"\"\"\n",
    "    current_time = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    cell_line_names = '-'.join(args.cell_line_list)\n",
    "    log_dir_name = f\"{current_time}_{cell_line_names}\"\n",
    "    log_dir = os.path.join('../logs_models/train_logs_models', log_dir_name)\n",
    "    \n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    \n",
    "    return log_dir\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def set_logger(args):\n",
    "    '''\n",
    "    Write logs to checkpoint and console \n",
    "    '''\n",
    "    log_dir = generate_log_dir(args)\n",
    "    log_file = os.path.join(log_dir, 'train.log') if args.do_train else os.path.join(log_dir, 'test.log')\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s %(levelname)-8s %(message)s', \n",
    "        level=logging.INFO,  # 设置日志级别为INFO\n",
    "        datefmt='%Y-%m-%d %H:%M:%S', \n",
    "        filename=log_file, \n",
    "        filemode='w'  # 每次运行时重写日志文件\n",
    "    )\n",
    "\n",
    "    console = logging.StreamHandler() \n",
    "    console.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s %(levelname)-8s %(message)s') \n",
    "    console.setFormatter(formatter) \n",
    "    logging.getLogger('').addHandler(console) \n",
    "    \n",
    "    return log_dir  # 返回日志目录以便其他地方使用\n",
    "\n",
    "\n",
    "def compute_accuracy(target, pred, pred_edge):\n",
    "    target = np.array(target)\n",
    "    pred = np.array(pred)\n",
    "    pred_edge = np.array(pred_edge)\n",
    "    \n",
    "    # 转换为 PyTorch 张量\n",
    "    pred_edge_tensor = torch.tensor(pred_edge, dtype=torch.float32)\n",
    "    scores = torch.softmax(pred_edge_tensor, dim=1).numpy()\n",
    "\n",
    "    \n",
    "    target = target.astype(int)\n",
    "    \n",
    "\n",
    "    \n",
    "    # 计算各项指标\n",
    "    aucu = roc_auc_score(target, scores[:, 1])\n",
    "    precision_tmp, recall_tmp, _thresholds = precision_recall_curve(target, scores[:, 1])\n",
    "    aupr = auc(recall_tmp, precision_tmp)\n",
    "    aupr= average_precision_score(target,scores[:,1])\n",
    "    f1 = f1_score(target, pred)\n",
    "    kappa = cohen_kappa_score(target, pred)\n",
    "    bacc = balanced_accuracy_score(target, pred)\n",
    "    \n",
    "    return aucu, aupr, f1, kappa, bacc\n",
    "\n",
    "\n",
    "def compute_accuracy_2(target,pred, pred_edge):\n",
    "    \n",
    "    target=target.clone().detach().cpu().numpy()\n",
    "    pred=pred.clone().detach().cpu().numpy()\n",
    "    pred_edge=pred_edge.clone().detach().cpu()\n",
    "    scores = torch.softmax(pred_edge, 1).numpy()\n",
    "    target=target.astype(int)\n",
    "    \n",
    "    print(target)\n",
    "    print(scores[:,1])\n",
    "   \n",
    "    aucu=roc_auc_score(target,scores[:,1])\n",
    "    precision_tmp, recall_tmp, _thresholds = precision_recall_curve(target, pred)\n",
    "    aupr = auc(recall_tmp, precision_tmp)\n",
    "    aupr= average_precision_score(target,scores[:,1])\n",
    "    f1 = f1_score(target,pred)\n",
    "    kappa=cohen_kappa_score(target,pred)\n",
    "    bacc=balanced_accuracy_score(target,pred)\n",
    "    \n",
    "    return aucu,aupr,f1,kappa,bacc\n",
    "\n",
    "\n",
    "def load_cell_line_gene_data(args, cell_line):\n",
    "    \"\"\"\n",
    "    load cell line specific gene data\n",
    "    \"\"\"\n",
    "    cell_line_gene_data = pd.read_csv(f\"{args.processed_data_path}/{cell_line}_all_data_gene.csv\")\n",
    "    return cell_line_gene_data\n",
    "\n",
    "\n",
    "\n",
    "def load_esm_embedding_data(args, node_index_data):\n",
    "    esm_embedding = joblib.load(args.esm_embedding_file )\n",
    "    esm_embedding_geneid = {}\n",
    "    for key, value in esm_embedding.items():\n",
    "        if key not in node_index_data['gene/protein']:\n",
    "            mapped_key = key  # Use original key or a placeholder if needed\n",
    "            esm_embedding_geneid[mapped_key] = torch.zeros(1280)\n",
    "        else:\n",
    "            mapped_key = node_index_data['gene/protein'][key]\n",
    "            esm_embedding_geneid[mapped_key] = value\n",
    "    return esm_embedding_geneid\n",
    "\n",
    "\n",
    "def Downstream_data_preprocess(args,n_fold,node_type_dict,cell_line): #FIXME\n",
    "    \"\"\"\n",
    "    load SL data and preprocess before training \n",
    "    \"\"\"\n",
    "    task_data_path=args.Task_data_path\n",
    "    train_data=pd.read_csv(f\"{task_data_path}/{cell_line}/train_{n_fold}.csv\")\n",
    "    test_data=pd.read_csv(f\"{task_data_path}/{cell_line}/valid_{n_fold}.csv\",)\n",
    "    train_data.columns=[0,1,2,3]\n",
    "    test_data.columns=[0,1,2,3]\n",
    "    train_data[0]=train_data[0].astype(str).map(node_type_dict)\n",
    "    train_data[1]=train_data[1].astype(str).map(node_type_dict)\n",
    "    test_data[0]=test_data[0].astype(str).map(node_type_dict)\n",
    "    test_data[1]=test_data[1].astype(str).map(node_type_dict)\n",
    "    train_data=train_data.dropna()\n",
    "    test_data=test_data.dropna()\n",
    "    train_data[0]=train_data[0].astype(int)\n",
    "    train_data[1]=train_data[1].astype(int)\n",
    "    test_data[0]=test_data[0].astype(int)\n",
    "    test_data[1]=test_data[1].astype(int)\n",
    "    # low data scenario settings\n",
    "    if args.do_low_data:\n",
    "        num_sample=int(train_data.shape[0]*args.train_data_ratio)\n",
    "        print(num_sample)\n",
    "        train_data=train_data.sample(num_sample,replace=False,random_state=0)\n",
    "        train_data.reset_index(inplace=True)\n",
    "        print(f'train_data.size:{train_data.shape[0]}')\n",
    "\n",
    "    train_node=list(set(train_data[0])|set(train_data[1]))\n",
    "    print(f'train_node.size:{len(train_node)}')\n",
    "    train_mask=torch.zeros((27671))\n",
    "    test_mask=torch.zeros((27671))\n",
    "    test_node=list(set(test_data[0])|set(test_data[1]))\n",
    "    train_mask[train_node]=1\n",
    "    test_mask[test_node]=1\n",
    "    train_mask=train_mask.bool()\n",
    "    test_mask=test_mask.bool()\n",
    "    num_train_node=len(train_node)\n",
    "    num_test_node=len(test_node)\n",
    "    return train_data,test_data,train_mask,test_mask,num_train_node,num_test_node\n",
    "\n",
    "def override_config(args):\n",
    "    '''\n",
    "    Override model and data configuration \n",
    "    '''\n",
    "    with open(os.path.join(args.init_checkpoint, 'config.json'), 'r') as fjson:\n",
    "        argparse_dict = json.load(fjson)\n",
    "    \n",
    "    args.method=argparse_dict['method']\n",
    "    # args.epochs = argparse_dict['epochs']\n",
    "    args.lr = argparse_dict['lr']\n",
    "    args.num_layer = argparse_dict['num_layer']\n",
    "    args.emb_dim = argparse_dict['emb_dim']\n",
    "    args.mask_rate = argparse_dict['mask_rate']\n",
    "    args.gnn_type=argparse_dict['gnn_type']\n",
    "\n",
    "    if args.Save_model_path is None:\n",
    "        args.Save_model_path = argparse_dict['Save_model_path']\n",
    "\n",
    "class GenePairDataset(Dataset):\n",
    "    def __init__(self, gene_pairs: pd.DataFrame):\n",
    "        # drop column 2\n",
    "        self.gene_pairs = gene_pairs.drop(columns=2).values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.gene_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.gene_pairs[idx]\n",
    "\n",
    "\n",
    "class sequence_dataset(Dataset):\n",
    "    def __init__(self,sequence_data):\n",
    "        self.sequence_data=sequence_data\n",
    "    def __len__(self):\n",
    "        return len(self.sequence_data)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.sequence_data[idx]\n",
    "\n",
    "def Construct_loader(args,kgdata,cell_line_gene_data,sequence_data,train_mask,test_mask,node_type,train_batch_size,test_batch_size):\n",
    "    \"\"\"\n",
    "    construct loader for train/test data\n",
    "    \"\"\"\n",
    "    \n",
    "    train_loader = HGTLoader(kgdata,\n",
    "    num_samples={key: [args.sample_nodes] * args.sample_layers for key in kgdata.node_types},shuffle=False,\n",
    "    batch_size=train_batch_size,\n",
    "    input_nodes=(node_type,train_mask),num_workers=args.num_workers)\n",
    "\n",
    "    return train_loader\n",
    "\n",
    "\n",
    "\n",
    "# HGT Model with a classification head\n",
    "class HGT4Classification(nn.Module):\n",
    "    def __init__(self,args, hgt,emb_dim,hidden_dim, num_classes, len_unique_node):\n",
    "        super(HGT4Classification, self).__init__()\n",
    "        self.hgt = hgt\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "        self.args = args\n",
    "    \n",
    "    def forward(self, kg_batch,batch):\n",
    "        node_rep = self.hgt(kg_batch.x_dict, kg_batch.edge_index_dict)\n",
    "        node_rep=   node_rep[args.node_type]\n",
    "        node_set=pd.DataFrame(list(kg_batch[node_type].n_id[:len_unique_node].squeeze().detach().cpu().numpy()))\n",
    "        node_set.drop_duplicates(inplace=True,keep='first')\n",
    "        node_set[1]=range(node_set.shape[0])\n",
    "        node_map=dict(zip(node_set[0],node_set[1]))\n",
    "        batch=pd.DataFrame(batch.numpy())\n",
    "        prediction_edge=batch[[0,1]]\n",
    "        prediction_label=batch[2]\n",
    "        edge_a,edge_b=prediction_edge[0],prediction_edge[1]\n",
    "        edge_a=edge_a.map(node_map)\n",
    "        edge_b=edge_b.map(node_map)\n",
    "        HGT_nodea_emb=node_rep[edge_a.values]\n",
    "        HGT_nodeb_emb=node_rep[edge_b.values]\n",
    "        edge_embedding = torch.cat([HGT_nodea_emb, HGT_nodeb_emb], dim=1)\n",
    "        emb_dim = edge_embedding.size(1)\n",
    "        pred = self.mlp(edge_embedding)\n",
    "        return pred\n",
    "        \n",
    "class HGT_ESM_4Classification(nn.Module):\n",
    "    def __init__(self,args, hgt,emb_dim,hidden_dim, num_classes, len_unique_node):\n",
    "        super(HGT_ESM_4Classification, self).__init__()\n",
    "        self.hgt = hgt\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(768, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        self.args = args\n",
    "        self.esm_linear_a = nn.Linear(1280, 256)\n",
    "        self.esm_linear_b = nn.Linear(1280, 256)\n",
    "    def forward(self, kg_batch,batch, ESM_nodea_emb, ESM_nodeb_emb):\n",
    "        node_rep = self.hgt(kg_batch.x_dict, kg_batch.edge_index_dict)\n",
    "        node_rep=   node_rep[args.node_type]\n",
    "        node_set=pd.DataFrame(list(kg_batch[node_type].n_id[:len_unique_node].squeeze().detach().cpu().numpy()))\n",
    "        node_set.drop_duplicates(inplace=True,keep='first')\n",
    "        node_set[1]=range(node_set.shape[0])\n",
    "        node_map=dict(zip(node_set[0],node_set[1]))\n",
    "        batch=pd.DataFrame(batch.numpy())\n",
    "        prediction_edge=batch[[0,1]]\n",
    "        prediction_label=batch[2]\n",
    "        edge_a,edge_b=prediction_edge[0],prediction_edge[1]\n",
    "        edge_a=edge_a.map(node_map)\n",
    "        edge_b=edge_b.map(node_map)\n",
    "        HGT_nodea_emb=node_rep[edge_a.values]\n",
    "        HGT_nodeb_emb=node_rep[edge_b.values]\n",
    "        ESM_nodea_emb = self.esm_linear_a(ESM_nodea_emb)\n",
    "        ESM_nodeb_emb = self.esm_linear_b(ESM_nodeb_emb)\n",
    "        edge_embedding = torch.cat([HGT_nodea_emb, HGT_nodeb_emb, ESM_nodea_emb, ESM_nodeb_emb], dim=1)        \n",
    "        emb_dim = edge_embedding.size(1)\n",
    "        pred = self.mlp(edge_embedding) \n",
    "        return pred\n",
    "        \n",
    "class HGT_ESM_Attention_4Classification(nn.Module):\n",
    "    def __init__(self, args, hgt):\n",
    "        super(HGT_ESM_Attention_4Classification, self).__init__()\n",
    "        self.hgt = hgt\n",
    "        self.args = args\n",
    "        self.esm_linear_a = nn.Linear(1280, 256)\n",
    "        self.esm_linear_b = nn.Linear(1280, 256)\n",
    "        \n",
    "        # Attention Layer\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=512 + 256, num_heads=8)\n",
    "        \n",
    "        # Final Linear Layer\n",
    "        self.fc = nn.Linear(512 + 256, 2)\n",
    "        \n",
    "    def forward(self, kg_batch, batch, ESM_nodea_emb, ESM_nodeb_emb):\n",
    "        node_rep = self.hgt(kg_batch.x_dict, kg_batch.edge_index_dict)\n",
    "        node_rep = node_rep[self.args.node_type]\n",
    "        \n",
    "        node_set = pd.DataFrame(list(kg_batch[self.args.node_type].n_id[:len_unique_node].squeeze().detach().cpu().numpy()))\n",
    "        node_set.drop_duplicates(inplace=True, keep='first')\n",
    "        node_set[1] = range(node_set.shape[0])\n",
    "        node_map = dict(zip(node_set[0], node_set[1]))\n",
    "        \n",
    "        batch = pd.DataFrame(batch.numpy())\n",
    "        prediction_edge = batch[[0, 1]]\n",
    "        prediction_label = batch[2]\n",
    "        \n",
    "        edge_a, edge_b = prediction_edge[0], prediction_edge[1]\n",
    "        edge_a = edge_a.map(node_map)\n",
    "        edge_b = edge_b.map(node_map)\n",
    "        \n",
    "        HGT_nodea_emb = node_rep[edge_a.values]\n",
    "        HGT_nodeb_emb = node_rep[edge_b.values]\n",
    "        \n",
    "        ESM_nodea_emb = self.esm_linear_a(ESM_nodea_emb)\n",
    "        ESM_nodeb_emb = self.esm_linear_b(ESM_nodeb_emb)\n",
    "        \n",
    "        edge_embedding = torch.cat([HGT_nodea_emb, HGT_nodeb_emb, ESM_nodea_emb, ESM_nodeb_emb], dim=1)\n",
    "        \n",
    "        # Reshape for MultiheadAttention (batch_size, seq_length, embedding_dim)\n",
    "        edge_embedding = edge_embedding.unsqueeze(0)  # Add batch dimension\n",
    "        attn_output, _ = self.attention(edge_embedding, edge_embedding, edge_embedding)\n",
    "        attn_output = attn_output.squeeze(0)  # Remove batch dimension\n",
    "        \n",
    "        pred = self.fc(attn_output)\n",
    "        return pred\n",
    "\n",
    "class HGT_ESM_CLdata_4Classification(nn.Module):\n",
    "    def __init__(self,args,hgt):\n",
    "        super(HGT_ESM_CLdata_4Classification, self).__init__()\n",
    "        self.hgt = hgt\n",
    "        self.esm_linear_a = nn.Linear(1280, 256)\n",
    "        self.esm_linear_b = nn.Linear(1280, 256)\n",
    "        self.fc1 = nn.Linear(768, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128 + 6, 2)\n",
    "        self.args = args\n",
    "\n",
    "    def forward(self, kg_batch,batch, ESM_nodea_emb, ESM_nodeb_emb,cell_line_gene_data_nodea, cell_line_gene_data_nodeb):\n",
    "        node_rep = self.hgt(kg_batch.x_dict, kg_batch.edge_index_dict)\n",
    "        node_rep=   node_rep[args.node_type]\n",
    "        node_set=pd.DataFrame(list(kg_batch[node_type].n_id[:len_unique_node].squeeze().detach().cpu().numpy()))\n",
    "        node_set.drop_duplicates(inplace=True,keep='first')\n",
    "        node_set[1]=range(node_set.shape[0])\n",
    "        node_map=dict(zip(node_set[0],node_set[1]))\n",
    "        batch=pd.DataFrame(batch.numpy())\n",
    "        prediction_edge=batch[[0,1]]\n",
    "        prediction_label=batch[2]\n",
    "        edge_a,edge_b=prediction_edge[0],prediction_edge[1]\n",
    "        edge_a=edge_a.map(node_map)\n",
    "        edge_b=edge_b.map(node_map)\n",
    "        HGT_nodea_emb=node_rep[edge_a.values]\n",
    "        HGT_nodeb_emb=node_rep[edge_b.values]\n",
    "        ESM_nodea_emb = self.esm_linear_a(ESM_nodea_emb)\n",
    "        ESM_nodeb_emb = self.esm_linear_b(ESM_nodeb_emb)\n",
    "        edge_embedding = torch.cat([HGT_nodea_emb, HGT_nodeb_emb, ESM_nodea_emb, ESM_nodeb_emb], dim=1)   \n",
    "        edge_embedding = self.fc1(edge_embedding)\n",
    "        edge_embedding = self.relu(edge_embedding)\n",
    "        edge_embedding = torch.cat([edge_embedding, cell_line_gene_data_nodea, cell_line_gene_data_nodeb], dim=1)\n",
    "        pred = self.fc2(edge_embedding)\n",
    "        return pred\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if inputs.dim() > 1:\n",
    "            # 多分类问题，使用 softmax 将 logits 转换为概率\n",
    "            probs = F.softmax(inputs, dim=1)\n",
    "            # 获取每个样本的正确类的概率\n",
    "            targets_one_hot = F.one_hot(targets, num_classes=inputs.size(1))\n",
    "            targets_one_hot = targets_one_hot.type_as(inputs)\n",
    "            pt = torch.sum(probs * targets_one_hot, dim=1)\n",
    "        else:\n",
    "            # 二分类问题，使用 sigmoid 将 logits 转换为概率\n",
    "            probs = torch.sigmoid(inputs)\n",
    "            pt = torch.where(targets == 1, probs, 1 - probs)\n",
    "        \n",
    "        # 计算交叉熵损失\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none') if inputs.dim() > 1 else F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        \n",
    "        # 计算 Focal Loss\n",
    "        loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        # 根据 reduction 参数返回结果\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "def create_optimizer(model, base_lr, fc_lr, base_weight_decay, fc_weight_decay):\n",
    "    # 创建参数组\n",
    "    param_groups = [\n",
    "        {\n",
    "            'params': model.hgt.parameters(), \n",
    "            'lr': base_lr,\n",
    "            'weight_decay': base_weight_decay\n",
    "        },\n",
    "        {\n",
    "            'params': model.esm_linear_a.parameters(), \n",
    "            'lr': fc_lr,\n",
    "            'weight_decay': fc_weight_decay\n",
    "        },\n",
    "        {\n",
    "            'params': model.esm_linear_b.parameters(), \n",
    "            'lr': fc_lr,\n",
    "            'weight_decay': fc_weight_decay\n",
    "        },\n",
    "        {\n",
    "            'params': model.fc1.parameters(), \n",
    "            'lr': fc_lr,\n",
    "            'weight_decay': fc_weight_decay\n",
    "        },\n",
    "        {\n",
    "            'params': model.fc2.parameters(), \n",
    "            'lr': fc_lr,\n",
    "            'weight_decay': fc_weight_decay\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # 创建优化器，使用不同的参数组和学习率\n",
    "    optimizer = optim.Adam(param_groups)\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = set_logger(args)\n",
    "logger = logging.getLogger('')  # 获取默认日志记录器\n",
    "with open (args.Full_data_path,'rb') as f:\n",
    "    kgdata=pickle.load(f)\n",
    "    \n",
    "logger.info(\"Loaded kgdata from {}\".format(args.Full_data_path))\n",
    "\n",
    "with open(\"../data/processed_data/gene_protein_2_id.json\",'rb') as f:\n",
    "    node_index=json.load(f)\n",
    "sequence_data = pd.read_csv('../data/train_data/uniprot_results_filtered.csv')\n",
    "sequence_data['Gene_id'] = sequence_data['Gene Name'].map(node_index['gene/protein'])\n",
    "if args.init_checkpoint:  \n",
    "    override_config(args)\n",
    "\n",
    "esm_embedding_geneid = load_esm_embedding_data(args, node_index)\n",
    "logger.info(\"Loaded ESM embeddings from {}\".format(args.esm_embedding_file))\n",
    "\n",
    "    \n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "device = torch.device(args.device_1 )\n",
    "device_0 = torch.device(args.device_0)\n",
    "device_1 = torch.device(args.device_1)\n",
    "device_2 = torch.device(args.device_2)\n",
    "device_3 = torch.device(args.device_3)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    \n",
    "gene_protein=node_index[args.node_type] \n",
    "eval_metric_folds={'fold':[],'auc':[],'aupr':[],'f1':[],'bacc':[],'kappa':[]}\n",
    "node_type=args.node_type\n",
    "num_nodes_type=len(kgdata.node_types)\n",
    "num_edge_type=len(kgdata.edge_types)\n",
    "num_nodes=kgdata.num_nodes\n",
    "input_node_embeddings = torch.nn.Embedding(num_nodes_type, 16)\n",
    "torch.nn.init.xavier_uniform_(input_node_embeddings.weight.data)\n",
    "for i in range(len(kgdata.node_types)):\n",
    "    num_repeat=kgdata[kgdata.node_types[i]].x.shape[0]\n",
    "    kgdata[kgdata.node_types[i]].x =input_node_embeddings(torch.tensor(i)).repeat([num_repeat,1]).detach()\n",
    "\n",
    "\n",
    "\n",
    "eval_metric_folds = {'fold':[], 'auc':[], 'aupr':[], 'f1':[], 'bacc':[], 'kappa':[]}\n",
    "\n",
    "\n",
    "criterion = FocalLoss()\n",
    "logging.info(f\"Using Criterion: FocalLoss\")\n",
    "\n",
    "logging.info(f\"Cell line specific\")\n",
    "for cell_line in args.cell_line_list:\n",
    "    cell_line_log_dir = os.path.join(log_dir, cell_line)\n",
    "    if not os.path.exists(cell_line_log_dir):\n",
    "        os.makedirs(cell_line_log_dir)\n",
    "    cell_line_gene_data = load_cell_line_gene_data(args, cell_line) \n",
    "    cell_line_gene_data['Gene_id'] = cell_line_gene_data['Gene Name'].map(node_index['gene/protein'])   \n",
    "    logger.info(f\"Loaded {cell_line} gene data from {args.Task_data_path}\")\n",
    "    for fold in range(args.folds):\n",
    "        n_fold = fold\n",
    "        fold_log_dir = os.path.join(cell_line_log_dir, f'fold_{fold}')\n",
    "        if not os.path.exists(fold_log_dir):\n",
    "            os.makedirs(fold_log_dir)\n",
    "        best_auc = 0\n",
    "        \n",
    "        HGT_model = HGT(kgdata,2*args.hgt_emb_dim,args.hgt_emb_dim,args.hgt_num_heads,args.hgt_num_layer).to(args.device)\n",
    "\n",
    "        if args.init_checkpoint: \n",
    "            # Restore model from checkpoint directory  \n",
    "            logging.info('Loading checkpoint %s...' % args.init_checkpoint)\n",
    "            checkpoint = torch.load(os.path.join(args.init_checkpoint, 'checkpoint'))\n",
    "            init_step = checkpoint['step']\n",
    "            HGT_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            \n",
    "        HGT_ESM_CLdata_4Classification_model = HGT_ESM_CLdata_4Classification(args, HGT_model).to(args.device)\n",
    "        logging.info(f\"Using HGT_ESM_CLdata_4Classification_model\")\n",
    "        \n",
    "        # optimizer_model = optim.Adam(HGT_ESM_CLdata_4Classification_model.parameters(), lr=args.lr, weight_decay=args.decay)\n",
    "        optimizer_model = create_optimizer(HGT_ESM_CLdata_4Classification_model,args.hgt_lr, args.fc_lr, args.base_weight_decay, args.fc_weight_decay)\n",
    "        \n",
    "        train_data,test_data,train_mask,test_mask,num_train_node,num_test_node=Downstream_data_preprocess(args,n_fold,gene_protein,cell_line)\n",
    "        best_metrics = {'auc': 0, 'aupr': 0, 'f1': 0, 'bacc': 0, 'kappa': 0}\n",
    "        loss_sum = 0\n",
    "        aucu_sum=0\n",
    "        f1_sum=0\n",
    "        bacc_sum=0\n",
    "        kappa_sum=0\n",
    "        aupr_sum=0\n",
    "        edge_used=[]\n",
    "            # map gene name(column name) to gene id\n",
    "        training_logs = []\n",
    "        testing_logs=[]\n",
    "        prediction_result_log_fold=[]\n",
    "        label_log_fold = []\n",
    "    \n",
    "        auc_sum_fold=[]\n",
    "        aupr_sum_fold=[]\n",
    "        f1_sum_fold=[]\n",
    "        bacc_sum_fold=[]\n",
    "        kappa_sum_fold=[]\n",
    "        \n",
    "        best_model_path = os.path.join(log_dir, 'best_model.pth')\n",
    "        \n",
    "        logger.info(f\"Training {cell_line} fold {fold}\")\n",
    "        \n",
    "        for epoch in tqdm(range(args.epoch)):\n",
    "            HGT_ESM_CLdata_4Classification_model.train()\n",
    "            gene_pair_loader = DataLoader(GenePairDataset(train_data), batch_size=args.train_batch_size, shuffle=True)\n",
    "            # Train\n",
    "            prediction_result_log_epoch=[]\n",
    "            label_log_epoch = []\n",
    "            loss_sum = 0\n",
    "            for step,batch in enumerate(tqdm(gene_pair_loader)):\n",
    "                optimizer_model.zero_grad()\n",
    "                node_a = batch[:, 0]\n",
    "                node_b = batch[:, 1]\n",
    "                node = torch.cat([node_a, node_b], dim=0)\n",
    "                label = batch[:, 2].to(args.device)\n",
    "                node_set = set(node_a.numpy()) | set(node_b.numpy())\n",
    "                unique_node = list(node_set)\n",
    "                len_unique_node = len(unique_node)\n",
    "                node_mask = torch.zeros((27671)) # The number of gene/protein nodes in kg\n",
    "                node_mask[unique_node] = 1\n",
    "                node_mask = node_mask.bool()\n",
    "                \n",
    "                kg_loader = HGTLoader(kgdata,\n",
    "                    num_samples={key: [args.sample_nodes] * args.sample_layers for key in kgdata.node_types},\n",
    "                    shuffle=False,\n",
    "                    batch_size=len_unique_node,\n",
    "                    input_nodes=(node_type,node_mask),\n",
    "                    num_workers=args.num_workers) \n",
    "                for kg_batch in kg_loader:\n",
    "                    break\n",
    "                kg_batch.to(args.device)\n",
    "                ESM_nodea_emb = torch.stack([esm_embedding_geneid[one_node.item()] for one_node in node_a]).to(args.device)\n",
    "                ESM_nodeb_emb = torch.stack([esm_embedding_geneid[one_node.item()] for one_node in node_b]).to(args.device)\n",
    "                \n",
    "                cell_line_gene_data_nodea = []\n",
    "                for one_node in node_a:\n",
    "                    selected_data = cell_line_gene_data.loc[cell_line_gene_data['Gene_id'] == one_node.item(), ['CN', 'Expression', 'HotspotMutation']]\n",
    "                    cell_line_gene_data_nodea.append(selected_data.values.tolist())\n",
    "                cell_line_gene_data_nodea_embedding = torch.tensor(np.array(cell_line_gene_data_nodea).squeeze())\n",
    "                cell_line_gene_data_nodeb = []\n",
    "                for one_node in node_b:\n",
    "                    selected_data = cell_line_gene_data.loc[cell_line_gene_data['Gene_id'] == one_node.item(), ['CN', 'Expression', 'HotspotMutation']]\n",
    "                    cell_line_gene_data_nodeb.append(selected_data.values.tolist())\n",
    "                cell_line_gene_data_nodeb_embedding = torch.tensor(np.array(cell_line_gene_data_nodeb).squeeze())\n",
    "                \n",
    "                cell_line_gene_data_nodea_embedding = cell_line_gene_data_nodea_embedding.float().to(args.device)\n",
    "                cell_line_gene_data_nodeb_embedding = cell_line_gene_data_nodeb_embedding.float().to(args.device)\n",
    "                \n",
    "                prediction_result = HGT_ESM_CLdata_4Classification_model(kg_batch,batch,ESM_nodea_emb,ESM_nodeb_emb, cell_line_gene_data_nodea_embedding, cell_line_gene_data_nodeb_embedding)\n",
    "                \n",
    "\n",
    "\n",
    "                label = label.long()\n",
    "                loss = criterion(prediction_result, label)\n",
    "                loss.backward()\n",
    "                loss_sum += loss.item()\n",
    "                torch.nn.utils.clip_grad_norm_(HGT_ESM_CLdata_4Classification_model.parameters(), max_norm=1.0)\n",
    "                optimizer_model.step()\n",
    "            logger.info(f\"Epoch {epoch}, Loss: {loss_sum}\")\n",
    "            \n",
    "            # valid\n",
    "            HGT_ESM_CLdata_4Classification_model.eval()\n",
    "            gene_pair_loader = DataLoader(GenePairDataset(test_data), batch_size=args.test_batch_size, shuffle=False)\n",
    "            aucu_sum = 0\n",
    "            f1_sum=0\n",
    "            bacc_sum=0\n",
    "            kappa_sum=0\n",
    "            aupr_sum=0\n",
    "            edge_used=[]\n",
    "            with torch.no_grad():\n",
    "                for step,batch in enumerate(tqdm(gene_pair_loader)):\n",
    "                    node_a = batch[:, 0]\n",
    "                    node_b = batch[:, 1]\n",
    "                    node = torch.cat([node_a, node_b], dim=0)\n",
    "                    label = batch[:, 2].to(args.device)\n",
    "                    node_set = set(node_a.numpy()) | set(node_b.numpy())\n",
    "                    unique_node = list(node_set)\n",
    "                    len_unique_node = len(unique_node)\n",
    "                    node_mask = torch.zeros((27671))\n",
    "                    node_mask[unique_node] = 1\n",
    "                    node_mask = node_mask.bool()\n",
    "                    \n",
    "                    kg_loader = HGTLoader(kgdata,\n",
    "                        num_samples={key: [args.sample_nodes] * args.sample_layers for key in kgdata.node_types},\n",
    "                        shuffle=False,\n",
    "                        batch_size=len_unique_node,\n",
    "                        input_nodes=(node_type,node_mask),\n",
    "                        num_workers=args.num_workers)\n",
    "                    \n",
    "                    for kg_batch in kg_loader:\n",
    "                        break\n",
    "                    kg_batch.to(args.device)\n",
    "                    \n",
    "                    ESM_nodea_emb = torch.stack([esm_embedding_geneid[one_node.item()] for one_node in node_a]).to(args.device)\n",
    "                    ESM_nodeb_emb = torch.stack([esm_embedding_geneid[one_node.item()] for one_node in node_b]).to(args.device)\n",
    "                    \n",
    "                    cell_line_gene_data_nodea = []\n",
    "                    for one_node in node_a:\n",
    "                        selected_data = cell_line_gene_data.loc[cell_line_gene_data['Gene_id'] == one_node.item(), ['CN', 'Expression', 'HotspotMutation']]\n",
    "                        cell_line_gene_data_nodea.append(selected_data.values.tolist())\n",
    "                    cell_line_gene_data_nodea_embedding = torch.tensor(np.array(cell_line_gene_data_nodea).squeeze())\n",
    "                    cell_line_gene_data_nodeb = []\n",
    "                    for one_node in node_b:\n",
    "                        selected_data = cell_line_gene_data.loc[cell_line_gene_data['Gene_id'] == one_node.item(), ['CN', 'Expression', 'HotspotMutation']]\n",
    "                        cell_line_gene_data_nodeb.append(selected_data.values.tolist())\n",
    "                    cell_line_gene_data_nodeb_embedding = torch.tensor(np.array(cell_line_gene_data_nodeb).squeeze())\n",
    "                    \n",
    "                    cell_line_gene_data_nodea_embedding = cell_line_gene_data_nodea_embedding.float().to(args.device)\n",
    "                    cell_line_gene_data_nodeb_embedding = cell_line_gene_data_nodeb_embedding.float().to(args.device)\n",
    "                    \n",
    "                    prediction_result = HGT_ESM_CLdata_4Classification_model(kg_batch,batch,ESM_nodea_emb,ESM_nodeb_emb, cell_line_gene_data_nodea_embedding, cell_line_gene_data_nodeb_embedding)\n",
    "                    prediction_result_log_epoch.append(prediction_result.detach().cpu().numpy())\n",
    "                    label_log_epoch.append(label.tolist())\n",
    "            prediction_result_log_epoch = np.concatenate(prediction_result_log_epoch)\n",
    "            label_log_epoch_flat = np.array(list(chain.from_iterable(label_log_epoch)))\n",
    "            \n",
    "\n",
    "\n",
    "            aucu, aupr, f1, kappa, bacc = compute_accuracy(label_log_epoch_flat, np.array(prediction_result_log_epoch).argmax(axis=1), prediction_result_log_epoch)\n",
    "            auc_sum_fold.append(aucu)\n",
    "            aupr_sum_fold.append(aupr)\n",
    "            f1_sum_fold.append(f1)\n",
    "            bacc_sum_fold.append(bacc)\n",
    "            kappa_sum_fold.append(kappa)\n",
    "            if aucu > best_metrics['auc']:\n",
    "                best_metrics = {'auc': aucu, 'aupr': aupr, 'f1': f1, 'bacc': bacc, 'kappa': kappa}\n",
    "                best_model_path = os.path.join(fold_log_dir, 'best_model.pth')\n",
    "                torch.save(HGT_ESM_CLdata_4Classification_model.state_dict(), best_model_path)\n",
    "            logger.info(f\"Epoch {epoch}, Loss: {loss_sum}, AUC: {aucu}, AUPR: {aupr}, F1: {f1}, Kappa: {kappa}, BAcc: {bacc}\")\n",
    "            \n",
    "        eval_metric_folds['fold'].append(fold)\n",
    "        eval_metric_folds['auc'].append(best_metrics['auc'])\n",
    "        eval_metric_folds['aupr'].append(best_metrics['aupr'])\n",
    "        eval_metric_folds['f1'].append(best_metrics['f1'])\n",
    "        eval_metric_folds['bacc'].append(best_metrics['bacc'])\n",
    "        eval_metric_folds['kappa'].append(best_metrics['kappa'])\n",
    "        \n",
    "        \n",
    "    avg_metrics = {key: np.mean(values) for key, values in eval_metric_folds.items() if key != 'fold'}\n",
    "    # cell line\n",
    "    logger.info(f\"{cell_line} Average Metrics:{avg_metrics}\")\n",
    "    best_metrics = {key: max(values) for key, values in eval_metric_folds.items() if key != 'fold'}\n",
    "    logger.info(f\"{cell_line} Best Metrics: {best_metrics}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioinfo_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
